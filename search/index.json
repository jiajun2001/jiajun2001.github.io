[{"content":"Many of the insights in this blog are inspired by Scott Meyers\u0026rsquo; Effective Modern C++, a must-read for any developer looking to deepen their understanding of C++11 and C++14.\nConstructors Parameterized Constructor Example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; class Animal { protected: std::string name; std::string species; public: Animal(const std::string\u0026amp; n, const std::string\u0026amp; s) : name(n), species(s) { std::cout \u0026lt;\u0026lt; \u0026#34;Animal constructor: Name = \u0026#34; \u0026lt;\u0026lt; name \u0026lt;\u0026lt; \u0026#34;, Species = \u0026#34; \u0026lt;\u0026lt; species \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } }; class Dog : public Animal { private: std::string breed; int age; public: // Dog constructor now passes both name and species to Animal Dog(const std::string\u0026amp; n, const std::string\u0026amp; s, const std::string\u0026amp; b, int a) : Animal(n, s), breed(b), age(a) { std::cout \u0026lt;\u0026lt; \u0026#34;Dog constructor: Breed = \u0026#34; \u0026lt;\u0026lt; breed \u0026lt;\u0026lt; \u0026#34;, Age = \u0026#34; \u0026lt;\u0026lt; age \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } }; int main() { Dog d(\u0026#34;Buddy\u0026#34;, \u0026#34;Canine\u0026#34;, \u0026#34;Golden Retriever\u0026#34;, 3); } /* output Animal constructor: Name = Buddy, Species = Canine Dog constructor: Breed = Golden Retriever, Age = 3 */ Copy Constructor Example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Buffer { private: int* data; public: Buffer(int value) { data = new int(value); } // Copy constructor (deep copy) Buffer(const Buffer\u0026amp; other) { data = new int(*other.data); // Allocate new memory std::cout \u0026lt;\u0026lt; \u0026#34;Deep copy constructor called\\n\u0026#34;; } ~Buffer() { delete data; } }; Purpose\ndefine a copy constructor when your class manages resources that require a deep copy Move Constructor Example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; class Resource { private: std::string* data; public: Resource(const std::string\u0026amp; val) : data(new std::string(val)) {} // Move constructor Resource(Resource\u0026amp;\u0026amp; other) noexcept : data(other.data) { other.data = nullptr; // leave moved-from object valid std::cout \u0026lt;\u0026lt; \u0026#34;Move constructor called\\n\u0026#34;; } ~Resource() { delete data; } void print() const { if (data) { std::cout \u0026lt;\u0026lt; *data \u0026lt;\u0026lt; std::endl; } else { std::cout \u0026lt;\u0026lt; \u0026#34;(empty)\u0026#34; \u0026lt;\u0026lt; std::endl; } } }; int main() { Resource r1(\u0026#34;hello\u0026#34;); Resource r2 = std::move(r1); // calls move constructor r2.print(); r1.print(); // should be empty or null } /* Move constructor called hello (empty) */ Purpose\nA move constructor is used to transfer (not copy) resources from a temporary object (rvalue) to a new object. It avoids expensive deep copies and enables performance optimizations. Explicit Constructor Example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include \u0026lt;iostream\u0026gt; class Foo { public: explicit Foo(int x) { std::cout \u0026lt;\u0026lt; \u0026#34;Foo(int)\\n\u0026#34;; } // no explicit here }; void bar(Foo f) {} int main() { Foo f1(10); // OK: direct initialization // Foo f2 = 10; // Error: copy initialization requires implicit conversion, which is disabled by explicit // bar(10); // Error: bar takes Foo, so int to Foo conversion needed (disabled) bar(Foo(10)); // OK: explicit constructor call also works } Purpose\nIt avoids unexpected conversions. Prevents bugs caused by accidental type conversions. Helps write clearer code by forcing you to be explicit when creating objects. Compiler Auto-generates \u0026hellip; 1 2 3 4 5 6 Point(); // default constructor Point(const Point\u0026amp;); // copy constructor Point(Point\u0026amp;\u0026amp;); // move constructor (C++11+) Point\u0026amp; operator=(const Point\u0026amp;); // copy assignment Point\u0026amp; operator=(Point\u0026amp;\u0026amp;); // move assignment ~Point(); // destructor Pointer to Const, Const Pointer and Reference Example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #include \u0026lt;iostream\u0026gt; using namespace std; int main(void) { // ---------- Pointer to Constant ---------- int a = 5; int b = 6; const int* ptrToConst = \u0026amp;a; ptrToConst = \u0026amp;b; // OKEY: You can change where the pointer points. // *ptrToConst = 20; // Error: You cannot change the value it points to. // ---------- Constant Pointer ---------- int* const constPtr = \u0026amp;a; *constPtr = 6; // OKEY: You can change the value it points to. // constPtr = \u0026amp;b; // ERROR: You cannot change where the pointer points. // ---------- Reference ---------- int\u0026amp; r = a; // Essentially, it is a constant pointer: int* const r = \u0026amp;a; } const function Definition: In C++, a const function is a member function that promises not to modify the object on which it is called. It is declared by placing the const keyword after the function’s parameter list. This is only relevant for member functions of classes or structs. Example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #include \u0026lt;iostream\u0026gt; class MyClass { int value; public: MyClass(int v) : value(v) {} int getValue() const { // value = 5; // Error: cannot modify member variable return value; } }; void printValue(const MyClass\u0026amp; obj) { std::cout \u0026lt;\u0026lt; obj.getValue(); } int main(void) { MyClass myClass(10); printValue(myClass); } Features:\nCannot modify any non-mutable member variables of the object. Can be called on const instances of the class. Can only call other const member functions. volatile keyword volatile tells the compiler:\n“Don’t assume this variable stays the same — its value might change at any time, even if your code doesn’t touch it.” Therefore:\nEvery time the variable is used, the compiler must reload it from memory It cannot cache the value in a register or optimize out reads/writes noexcept keyword noexcept tells the compiler:\n“This function is guaranteed not to throw any exceptions.” Usage:\nPerformance: Functions marked noexcept can enable compiler optimizations. Correctness: If a noexcept function does throw, the program will call std::terminate() and crash — this makes it clear something went wrong. STL Compatibility: The Standard Library often prefers or requires noexcept move constructors and destructors for performance. lvalue and rvalue An lvalue refers to an object that has a name and a memory address An rvalue is a temporary value that does not have a name and cannot be assigned to. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026lt;iostream\u0026gt; void check(int\u0026amp; x) { std::cout \u0026lt;\u0026lt; \u0026#34;Lvalue\\n\u0026#34;; } void check(int\u0026amp;\u0026amp; x) { std::cout \u0026lt;\u0026lt; \u0026#34;Rvalue\\n\u0026#34;; } int main() { int a = 10; check(a); // Lvalue check(20); // Rvalue check(std::move(a)); // Rvalue } Universal Reference A universal reference is a reference that can bind to both lvalues and rvalues. It’s written as T\u0026amp;\u0026amp; in a function template, where T is a deduced template parameter. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #include \u0026lt;iostream\u0026gt; template\u0026lt;typename T\u0026gt; void printType(T\u0026amp;\u0026amp; param) { if constexpr (std::is_lvalue_reference\u0026lt;decltype(param)\u0026gt;::value) // if constexpr: Like a regular if, but evaluated at compile time // decltype(param): This gives the exact type of the variable param, incl reference qualifiers std::cout \u0026lt;\u0026lt; \u0026#34;Lvalue\\n\u0026#34;; else std::cout \u0026lt;\u0026lt; \u0026#34;Rvalue\\n\u0026#34;; } int main() { int x = 10; printType(x); // Lvalue printType(42); // Rvalue printType(std::move(x)); // Rvalue } Case 1: lvalue T = int\u0026amp; (because x is an lvalue) param is of type int\u0026amp; \u0026amp;\u0026amp; → collapses to int\u0026amp; Case 2: rvalue T = int param is int\u0026amp;\u0026amp; Deducing Types 1 2 3 4 template\u0026lt;typename T\u0026gt; void f(ParamType param); f(expr); // call f with some expression The type deduced for T is dependent not just on the type of expr, but also on the form of ParamType. There are three cases. Three cases ParamType is a Reference or Pointer, but not a Universal Reference Rules:\nIf expr’s type is a reference, ignore the reference part. Then pattern-match expr’s type against ParamType to determine T. Examples:\n1 2 3 4 5 6 7 8 9 10 template\u0026lt;typename T\u0026gt; void f(T\u0026amp; param); // param is a reference int x = 27; // x is an int const int cx = x; // cx is a const int const int\u0026amp; rx = x; // rx is a reference to x as a const int f(x); // T is int, param\u0026#39;s type is int\u0026amp; f(cx); // T is const int, param\u0026#39;s type is const int\u0026amp; f(rx); // T is const int, param\u0026#39;s type is const int\u0026amp; Features:\nthe constness of the object becomes part of the type deduced for T the reference-ness is ignored during type deduction ParamType is a Universal Reference Rules:\nIf expr is an lvalue, both T and ParamType are deduced to be lvalue references If expr is an rvalue, the “normal” (i.e., Case 1) rules apply. Examples:\n1 2 3 4 5 6 7 8 9 10 11 template\u0026lt;typename T\u0026gt; void f(T\u0026amp;\u0026amp; param); // param is now a universal reference int x = 27; // as before const int cx = x; // as before const int\u0026amp; rx = x; // as before f(x); // x is lvalue, so T is int\u0026amp;, param\u0026#39;s type is also int\u0026amp; f(cx); // cx is lvalue, so T is const int\u0026amp;, param\u0026#39;s type is also const int\u0026amp; f(rx); // rx is lvalue, so T is const int\u0026amp;, param\u0026#39;s type is also const int\u0026amp; f(27); // 27 is rvalue, so T is int, param\u0026#39;s type is therefore int\u0026amp;\u0026amp; Features:\nwhen universal references are in use, type deduction distinguishes between lvalue arguments and rvalue arguments ParamType is Neither a Pointer nor a Reference Rules:\nAs before, if expr’s type is a reference, ignore the reference part. Ignoring expr’s reference-ness, and const-ness, volatile-ness. Examples:\n1 2 3 4 5 6 7 8 9 10 template\u0026lt;typename T\u0026gt; void f(T param); // param is now passed by value int x = 27; // as before const int cx = x; // as before const int\u0026amp; rx = x; // as before f(x); // T\u0026#39;s and param\u0026#39;s types are both int f(cx); // T\u0026#39;s and param\u0026#39;s types are again both int f(rx); // T\u0026#39;s and param\u0026#39;s types are still both int Features:\nconst (and volatile) is ignored only for by-value parameters An interesting case:\n1 2 3 4 5 6 7 8 9 10 11 12 template\u0026lt;typename T\u0026gt; void f(T param); // param is still passed by value const char* const ptr = \u0026#34;Fun with pointers\u0026#34;; // ptr is const pointer to const object // The rightmost const: ptr itself is const. // Once ptr is initialized, you cannot change what it points to. // The left const: the data being pointed to is const // We cannot modify the characters through ptr. f(ptr); // pass arg of type const char * const const-ness of ptr will be ignored, and the type deduced for param will be const char* The const-ness of what ptr points to is preserved during type deduction, but the const-ness of ptr itself is ignored when copying it to create the new pointer, param. Another interesting case\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 template\u0026lt;typename T\u0026gt; void f(const T\u0026amp; param) { // param is of type: const int* const\u0026amp; // 1) Attempting to modify the pointer itself (top-level const): // param = nullptr; // Compilation error: pointer itself is const // 2) Attempting to modify the value pointed to (low-level const): // *param = 100; // Compilation error: pointer points to const int std::cout \u0026lt;\u0026lt; \u0026#34;*param = \u0026#34; \u0026lt;\u0026lt; *param \u0026lt;\u0026lt; std::endl; // Prints 42 } int main() { const int x = 42; const int* p = \u0026amp;x; f(p); } T is deduced as const int*, so param becomes const T\u0026amp; → const (const int*)\u0026amp; First const → top-level const, applies to T itself → pointer cannot be reassigned (const int*) → pointer points to a const int → low-level const Merge repeated const: param : const int* const\u0026amp; // reference to const pointer to const int Array Arguments Example 1:\n1 2 3 4 5 6 7 template\u0026lt;typename T\u0026gt; void f(T param); // template with by-value parameter const char name[] = \u0026#34;J. P. Briggs\u0026#34;; const char * ptrToName = name; f(name); // name is array, but T deduced as const char* Feature 1:\nArray-to-pointer decay rule: array declaration is treated as a pointer declaration Example 2:\n1 2 3 4 5 6 template\u0026lt;typename T\u0026gt; void f(T\u0026amp; param); // template with by-reference parameter const char name[] = \u0026#34;J. P. Briggs\u0026#34;; f(name); // T is deduced as const char (\u0026amp;)[13] Feature 2:\nFunctions can declare parameters that are references to arrays. It enables creation of a template that deduces the number of elements that an array contains: Function Arguments Example\n1 2 3 4 5 6 7 8 9 void someFunc(int, double); // someFunc is a function, type is void(int, double) template\u0026lt;typename T\u0026gt; void f1(T param); // in f1, param passed by value template\u0026lt;typename T\u0026gt; void f2(T\u0026amp; param); // in f2, param passed by ref f1(someFunc); // param deduced as ptr-to-func, type is void (*)(int, double) f2(someFunc); // param deduced as ref-to-func, type is void (\u0026amp;)(int, double) Features:\nFunction-to-pointer decay rule. View Deducted Types by Compiler Diagnostics 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // declare a class template that we don’t define template\u0026lt;typename T\u0026gt; class TD; template\u0026lt;typename T\u0026gt; void f(const T\u0026amp; param) { TD\u0026lt;decltype(param)\u0026gt; obj; // This will cause a compilation error // compiler_diagnostics.cpp:7:25: // error: implicit instantiation of undefined template \u0026#39;TD\u0026lt;const int *const \u0026amp;\u0026gt;\u0026#39; } int main() { const int x = 42; const int* p = \u0026amp;x; f(p); } auto type deduction Equivalence Between Template Type Deduction and auto Type Deduction 1 2 3 4 template\u0026lt;typename T\u0026gt; void f(T); ⇔ auto x template\u0026lt;typename T\u0026gt; void f(T\u0026amp;); ⇔ auto\u0026amp; x template\u0026lt;typename T\u0026gt; void f(const T\u0026amp;); ⇔ const auto\u0026amp; x template\u0026lt;typename T\u0026gt; void f(T\u0026amp;\u0026amp;); ⇔ auto\u0026amp;\u0026amp; x A special type deduction rule for auto When the initializer for an auto-declared variable is enclosed in braces, the deduced type is always a std::initializer_list, but template type deduction doesn’t.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 using namespace std; template\u0026lt;typename T\u0026gt; void f1(T initList) {} template\u0026lt;typename T\u0026gt; void f2(initializer_list\u0026lt;T\u0026gt; initList) {} int main(void) { auto x = {1, 2, 3}; // x\u0026#39;s type is std::initializer_list\u0026lt;int\u0026gt; f1(x); // Works: C++ deduces x to be of type std::initializer_list\u0026lt;int\u0026gt; — this is special behavior for auto. // f1({1, 2, 3}); // Does not work: In template type deduction, {} by itself is not a type. f2({1, 2, 3}); // Works: it matches the special braced-initializer deduction rule } 2 cases when auto employs template type deduction auto as a return type 1 2 3 auto createInitList() { return { 1, 2, 3 }; // error: can\u0026#39;t deduce type for { 1, 2, 3 } } auto in a lambda parameter 1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;vector\u0026gt; int main(void) { std::vector\u0026lt;int\u0026gt; v; auto resetV = [\u0026amp;v](const auto\u0026amp; newValue) { v = newValue; }; // resetV({ 1, 2, 3 }); // Does not work: error! can\u0026#39;t deduce type for { 1, 2, 3 } auto x = {1, 2, 3}; resetV(x); // Works: auto deduces x to be of type std::initializer_list\u0026lt;int\u0026gt; } // -std=c++14 One Example showing template type deduction is working 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #include \u0026lt;iostream\u0026gt; // Lambda returning decltype(auto) → preserves reference \u0026amp; const auto get_ref = [](auto\u0026amp; x) -\u0026gt; auto\u0026amp; { return x; // preserves reference \u0026amp; constness }; int main() { const int ci = 42; int i = 10; // Using with a const int const int\u0026amp; r = get_ref(ci); // type deduced as const int\u0026amp; std::cout \u0026lt;\u0026lt; r \u0026lt;\u0026lt; std::endl; // 42 // r = 100; // ERROR: r is const // Using with a non-const int int\u0026amp; s = get_ref(i); // type deduced as int\u0026amp; s = 20; // modifies i std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; std::endl; // 20 return 0; } Understand decltype decltype in C++11 1 2 3 4 5 6 7 template\u0026lt;typename Container, typename Index\u0026gt; auto authAndAccess(Container\u0026amp; c, Index i) -\u0026gt; decltype(c[i]) { // In C++11, we must use a trailing return type if the return type depends on function parameters. // In C++11, auto is just a placeholder // In C++14, auto will deduce the return type return c[i]; } decltype(auto) 1 2 3 4 5 6 7 template\u0026lt;typename C, typename I\u0026gt; decltype(auto) authAndAccess(C\u0026amp; c, I i) { return c[i]; } // decltype(c[i]) → int\u0026amp; // if we use auto, auto -\u0026gt; int (lose reference-ness) decltype((x)) decltype(expr) only deduces a reference (T\u0026amp;) for non-name lvalue expressions.\nExamples of non-name lvalue expressions: v[i], *ptr, (x), obj.member\n1 2 3 4 5 6 7 8 9 10 11 decltype(auto) f1() { int x = 0; return x; // decltype(x) -\u0026gt; int } decltype(auto) f2() { int x = 0; return (x); // decltype((x)) -\u0026gt; int\u0026amp; // Undefined Behaviour // The lifetime of x ends as soon as the function returns } Prefer auto to explicit type declarations Auto variables must be initialized 1 2 auto x2; // error! initializer required auto x3 = 0; // fine, x\u0026#39;s value is well-defined Ability to directly hold closures Directly holding closures with auto is smaller, faster, and simpler.\nUsing auto\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 auto addX = [x](int y) { return x + y; }; struct __lambda_addX { int x; // captured variable int operator()(int y) const { return x + y; // direct access } }; // 1) Compiler generates a unique type for the lambda // -\u0026gt; Memory used = exactly what lambda needs (size of captured variables) // 2) Call is direct → can be inlined and is very fast Using std::function 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 std::function\u0026lt;int(int)\u0026gt; addXFunc = [x](int y) { return x + y; }; struct callable_base { virtual int call(int) = 0; virtual ~callable_base() {} }; template\u0026lt;typename F\u0026gt; struct callable_wrapper : callable_base { F f; // store the actual lambda callable_wrapper(F func) : f(func) {} int call(int y) override { return f(y); } }; // 1) Memory used = larger, may allocate on heap for bigger closures // 2) Call is indirect via virtual function /* addXFunc(5) -\u0026gt; std::function::operator()(5) -\u0026gt; ptr-\u0026gt;call(5) // virtual call -\u0026gt; callable_wrapper::call(5) -\u0026gt; lambda.operator()(5) -\u0026gt; 10 + 5 = 15 */ Less subtle errors Example 1: 1 2 3 4 unsigned sz = v.size(); // Returns std::vector\u0026lt;int\u0026gt;::size_type, // On 64-bit Windows, unsigned is 32 bits // While std::vector\u0026lt;int\u0026gt;::size_type is 64 bits. // Better to just use auto Example 2: 1 2 3 4 5 6 7 8 std::unordered_map\u0026lt;std::string, int\u0026gt; m; for (const std::pair\u0026lt;std::string, int\u0026gt;\u0026amp; p : m) { // do something with p } // unordered_map actually contains std::pair\u0026lt;const std::string, int\u0026gt; // C++ creates a temporary std::pair\u0026lt;std::string, int\u0026gt; by copying the key and value from the map’s std::pair\u0026lt;const std::string, int\u0026gt; Auto can deduce undesired types “Invisible” proxy types can cause auto to deduce the “wrong” type 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 // Proxy for delayed addition template\u0026lt;typename L, typename R\u0026gt; struct Sum { const L\u0026amp; left; const R\u0026amp; right; Sum(const L\u0026amp; l, const R\u0026amp; r) : left(l), right(r) {} int eval() const { return left.eval() + right.eval(); } }; // Base class for objects that can be evaluated struct Matrix { int value; // just a number for simplicity Matrix(int v = 0) : value(v) {} // Evaluate for a single matrix: just return its value int eval() const { return value; } // Construct Matrix from a proxy expression template\u0026lt;typename Expr\u0026gt; Matrix(const Expr\u0026amp; expr) { value = expr.eval(); // compute sum once } }; // Overload operator+ to return a proxy (delayed evaluation) template\u0026lt;typename L, typename R\u0026gt; Sum\u0026lt;L, R\u0026gt; operator+(const L\u0026amp; l, const R\u0026amp; r) { return Sum\u0026lt;L, R\u0026gt;(l, r); } int main() { // Safe: evaluates expression immediately into sum Matrix sum = Matrix(1) + Matrix(2) + Matrix(3) + Matrix(4); std::cout \u0026lt;\u0026lt; \u0026#34;sum = \u0026#34; \u0026lt;\u0026lt; sum.value \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; // sum = 10 // Dangerous: auto deduces \u0026#39;proxy\u0026#39; to be Sum\u0026lt;Sum\u0026lt;Sum\u0026lt;Matrix, Matrix\u0026gt;, Matrix\u0026gt;, Matrix\u0026gt; auto proxy = Matrix(1) + Matrix(2) + Matrix(3) + Matrix(4); // After the full expression, all the temporaries die. // Calling proxy.eval() dereferences dead memory → segmentation fault. std::cout \u0026lt;\u0026lt; \u0026#34;proxy.eval() = \u0026#34; \u0026lt;\u0026lt; proxy.eval() \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; // zsh: segmentation fault ./main } // clang++ deduct_wrong_type.cpp -o main -std=c++11 -O3 Distinguish between () and {} when creating objects Uniform Initialization: Introduced in c++11, a single initialization syntax that can, at least in concept, be used anywhere and express everything. It’s based on braces.\nUsage of Uniform Initialization Specify the initial contents of a container Example\n1 std::vector\u0026lt;int\u0026gt; v{ 1, 3, 5 }; // v\u0026#39;s initial content is 1, 3, 5 Prohibit implicit narrowing conversions among built-in types Example\n1 2 3 4 5 double x, y, z; int sum1{ x + y + z }; // error! sum of doubles may not be expressible as int int sum2(x + y + z); // okay (value of expression truncated to an int) Default-construct an object, not to declare a function Example\n1 2 Widget w2(); // most vexing parse! declares a function named w2 that returns a Widget! Widget w3{}; // calls Widget ctor with no args Drawback of Uniform Initialization Strong preference the call to the constructor of std::initializer_list (Constructor Overloading) 1 2 3 4 5 6 7 8 9 class Widget { public: Widget(int i, bool b); Widget(int i, double d); Widget(std::initializer_list\u0026lt;long double\u0026gt; il); } Widget w1(10, true); // uses parens and, as before, calls first ctor Widget w2{10, true}; // uses braces, but now calls std::initializer_list ctor (10 and true convert to long double) Prefer nullptr to 0 and null In C++98, both 0 and NULL are integral values, not pointers. Although the language allows them to be interpreted as null pointers in pointer contexts, this is only a fallback rule.\nAs a result, overloading functions on pointer and integral types can lead to surprising and counterintuitive behavior: passing 0 or NULL often selects an integer overload, never a pointer one.\nC++11 introduced nullptr to fix this problem. nullptr has its own type, std::nullptr_t, which is not an integral type but can implicitly convert to any pointer type.\nPrefer alias declarations to typedefs What is typedef?\ntypedef is about defining an alias. 1 2 3 4 5 6 template\u0026lt;typename T\u0026gt; struct Wrapper { typedef T value_type; }; Wrapper\u0026lt;int\u0026gt;::value_type x; // value_type == int What is typename?\nInside a template, typename tells the compiler that a dependent name refers to a type. It does NOT create a type.\nIf a name inside a template is a dependent qualified name and you want it to be treated as a type, you must prefix it with typename.\n1 2 3 4 template\u0026lt;typename T\u0026gt; void f() { typename T::value_type x; // T::value_type is a type (dependent type) — parse it as one. } The compiler cannot know yet whether value_type is a type or a static variable or an enum or something else What is alias declarations?\n1 2 using UPtrMapSS = std::unique_ptr\u0026lt;std::unordered_map\u0026lt;std::string, std::string\u0026gt;\u0026gt;; What is alias templates?\nWhen alias declarations are templatized 1 2 3 4 template\u0026lt;typename T\u0026gt; // MyAllocList\u0026lt;T\u0026gt; using MyAllocList = std::list\u0026lt;T, MyAlloc\u0026lt;T\u0026gt;\u0026gt;; // is synonym for std::list\u0026lt;T, MyAlloc\u0026lt;T\u0026gt;\u0026gt; MyAllocList\u0026lt;Widget\u0026gt; lw; // client code Declare overriding functions override Several requirements for overriding to occur The base class function must be virtual\nBase and derived functions must be identical in terms of: name / parameter types / constness / return types / exception specifications / reference qualifiers\nDeclare a derived class function to be override 1 2 3 4 5 6 7 class Derived: public Base { public: virtual void mf1() override; virtual void mf2(unsigned int x) override; virtual void mf3() \u0026amp;\u0026amp; override; virtual void mf4() const override; }; Use constexpr whenever possible Definition: constexpr is a C++ keyword used to indicate that a value or function can be evaluated at compile time. It helps write faster and safer code by enabling computations to be done during compilation rather than at runtime constexpr object Example\n1 2 3 4 5 6 int sz; // non-constexpr variable constexpr auto arraySize1 = sz; // error! sz\u0026#39;s value not known at compilation std::array\u0026lt;int, sz\u0026gt; data1; // error! same problem constexpr auto arraySize2 = 10; // fine, 10 is a compile-time constant std::array\u0026lt;int, arraySize2\u0026gt; data2; // fine, arraySize2 is constexpr Fearures:\nall constexpr objects are const, but not all const objects are constexpr, because const objects need not be initialized with values known during compilation. constexpr function Example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Point { public: constexpr Point(double xVal = 0, double yVal = 0) noexcept : x(xVal), y(yVal) { } constexpr double xValue() const noexcept { return x; } constexpr double yValue() const noexcept { return y; } constexpr void setX(double newX) noexcept { x = newX; } constexpr void setY(double newY) noexcept { y = newY; } private: double x, y; }; constexpr Point midpoint(const Point \u0026amp;p1, const Point \u0026amp;p2) noexcept { return {(p1.xValue() + p2.xValue()) / 2, // call constexpr (p1.yValue() + p2.yValue()) / 2}; // member funcs } int main(void) { constexpr Point p1(9.4, 27.7); // fine, \u0026#34;runs\u0026#34; constexpr ctor during compilation constexpr Point p2(28.8, 5.3); // also fine constexpr auto mid = midpoint(p1, p2); // init constexpr object w/result of constexpr function } // g++ constexpr.cpp -o constexpr -std=c++14 Features:\nconstexpr functions can be used in contexts that demand compile-time constants. When a constexpr function is called with one or more values that are not known during compilation, it acts like a normal function. Using constexpr v.s. not using constexpr:\nPrefer scoped enums to unscoped enums Unscoped Enum can pollute namespace 1 2 3 4 5 6 7 8 9 int main(void) { // ---------- Unscoped Enum can pollute namespace ---------- enum Color { white, black, red }; // auto white = 3; // error: redefinition of \u0026#39;white\u0026#39; as different kind of symbol // ---------- Scoped Enum can avoid namespace pollution ---------- enum class Animal { cat, dog, bird }; auto cat = 3; // OKEY } Scoped Enums are much more strongly typed 1 2 3 4 5 6 7 8 9 10 11 12 13 #include \u0026lt;iostream\u0026gt; using namespace std; int main(void) { enum UnscopedEnum {A}; enum class ScopedEnum {B}; // Unscoped enum values implicitly convert to their underlying integral type — usually int cout \u0026lt;\u0026lt; A \u0026lt;\u0026lt; endl; // ScopedEnum::B is not implicitly convertible to int cout \u0026lt;\u0026lt; static_cast\u0026lt;int\u0026gt;(ScopedEnum::B) \u0026lt;\u0026lt; endl; } Forward Declaration for Unscoped Enum 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #include \u0026lt;iostream\u0026gt; // Forward declaration of the enum // Specify the underlying type for an unscoped enum enum Color : int; void paint(Color c); // Use Color before its full definition // Full definition of the enum // Underlying type specifications should also go on an enum’s definition enum Color : int { Red, Green, Blue }; void paint(Color c) { switch (c) { case Red: std::cout \u0026lt;\u0026lt; \u0026#34;Red\\n\u0026#34;; break; case Green: std::cout \u0026lt;\u0026lt; \u0026#34;Green\\n\u0026#34;; break; case Blue: std::cout \u0026lt;\u0026lt; \u0026#34;Blue\\n\u0026#34;; break; } } int main() { paint(Green); return 0; } Lambda Capture Modes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #include \u0026lt;iostream\u0026gt; using namespace std; int main(void) { // ---------- Capture by value ---------- int x = 10; auto lambda_value = [x]() { cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; endl; // prints 10 }; lambda_value(); // ---------- Capture by reference ---------- auto lambda_reference = [\u0026amp;x]() { x += 10; }; lambda_reference(); cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; endl; // prints 20 } Default by-reference capture can lead to dangling reference 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #include \u0026lt;iostream\u0026gt; #include \u0026lt;functional\u0026gt; std::function\u0026lt;void()\u0026gt; make_lambda() { int x = 42; // Capture everything by reference return [\u0026amp;]() { std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl; // Dangling reference! }; } int main() { auto lambda_dangling_reference = make_lambda(); // x is destroyed when make_lambda() returns lambda_dangling_reference(); // prints 0, once make_lambda() returns, x is gone // its memory on the stack is invalid. // So calling the lambda later accesses garbage. return 0; } // Compile: g++ -O2 dangling_reference.cpp -o main -std=c++11 Default by-value capture is susceptible to dangling pointers 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include \u0026lt;iostream\u0026gt; #include \u0026lt;functional\u0026gt; std::function\u0026lt;void()\u0026gt; make_lambda() { int* ptr = new int(42); // dynamically allocated auto lambda = [ptr]() { // ptr is copied (by value), but it\u0026#39;s still a raw pointer std::cout \u0026lt;\u0026lt; *ptr \u0026lt;\u0026lt; std::endl; }; delete ptr; // pointer now dangles! return lambda; // lambda holds a dangling pointer } int main() { auto lambda = make_lambda(); lambda(); // dereferencing dangling pointer // prints 0 return 0; } Static and global variables cannot be captured 1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;iostream\u0026gt; int global_x = 42; auto lambda = []() { std::cout \u0026lt;\u0026lt; global_x \u0026lt;\u0026lt; std::endl; // accesses directly }; int main() { lambda(); // prints 42 } Smart Pointers std::unique_ptr std::unique_ptr embodies exclusive ownership semantics Moving a std::unique_ptr transfers ownership, while copying a std::unique_ptr isn’t allowed. std::shared_ptr std::weak_ptr Reference Many of the insights in this blog are inspired by Scott Meyers\u0026rsquo; Effective Modern C++, a must-read for any developer looking to deepen their understanding of C++11 and C++14. ","date":"2026-01-17T00:00:00Z","image":"https://jiajun2001.github.io/p/advanced-c-topics/picture_hu_a52802070a277cf8.jpg","permalink":"https://jiajun2001.github.io/p/advanced-c-topics/","title":"Advanced C++ Topics"},{"content":"The Six Prompt Components Task : Start with an action verb.\nContext: User background, what success looks like, and environment or conditions of user.\nExamples: Provide examples of the best or real results. It learns very fast through examples.\nPersona: Act like a ______. (Also add person famous for that).\nFormat: Tell the AI the right output format you want the answer in.\nTone: Specify the desired tone, whether casual, formal, enthusiastic, or specific feelings, guiding ChatGPT to match the writing style.\nReference ","date":"2026-01-16T00:00:00Z","image":"https://jiajun2001.github.io/p/prompt-engineering/image_hu_c24be03cbe48e216.png","permalink":"https://jiajun2001.github.io/p/prompt-engineering/","title":"Prompt Engineering"},{"content":"Introduction What is a game engine Software that is extensible and can be used as the foundation for many different games without major modification. Some Basic Concepts A view frustum is a pyramid-shaped volume representing the camera’s visible space, used to determine whether objects should be rendered based on whether they lie inside or outside this space.\nFrustum Culling is a fundamental visibility optimization that skips rendering objects completely outside the camera’s view frustum, reducing draw calls and CPU/GPU workload at minimal cost.\nOcclusion Culling is a visibility optimization technique that avoids rendering objects fully hidden behind other geometry, significantly reducing overdraw and GPU workload in complex scenes.\nA bounding volume is a simple geometric shape used to roughly represent a complex object, enabling fast visibility and collision decisions in a game engine, while a Bounding Sphere is one specific type of bounding volume shaped as a sphere.\nA Bounding Volume Hierarchy (BVH) is a tree structure of bounding volumes, allowing efficient visibility and collision tests by quickly excluding entire branches that are outside the camera view or occluded.\n1 2 3 4 5 BVH Root / \\ Node1 Node2 / \\ / \\ Obj1 Obj2 Obj3 Obj4 An Axis-Aligned Bounding Box (AABB) is a box-shaped bounding volume with edges aligned to the world axes, used for fast visibility and collision checks. When an object rotates, the AABB may need to enlarge to fully contain it, making the box appear “fatter.”\nHierarchical Z-Buffer (HZB) is a GPU optimization technique that builds a multi-resolution depth map to quickly determine if objects are occluded, reducing the need for per-pixel visibility tests.\nAn Oriented Bounding Box (OBB) is a box-shaped bounding volume that can rotate to align with an object, providing a tighter fit for precise collision detection.\nBasic Workflow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 +----------------+ | Camera | | View Frustum | +-------+--------+ | v +----------------------+ | Frustum Culling | | (Bounding Sphere) | +----------------------+ | v +----------------------+ | Occlusion Culling | | (AABB / OBB + HZB) | +----------------------+ | v +----------------------+ | Render List | +----------------------+ | v +-----------------+ | GPU Draw | +-----------------+ Reference Gregory, J. (2018). Game Engine Architecture (3rd ed.). CRC Press. ","date":"2026-01-14T14:49:06+10:30","image":"https://jiajun2001.github.io/p/game-engine-architecture/image_hu_63753cf4f9a6822d.png","permalink":"https://jiajun2001.github.io/p/game-engine-architecture/","title":"Game Engine Architecture"},{"content":"Process Management The process Linux does not differentiate between threads and process. Processes provide two virtualizations, a virtualized processor and virtual memory. Execution Workflow: fork(): creates a new process by duplicating an existing one exec(): creates a new address space and loads a new program into it exit(): terminates the process and frees all its resources. When a process exits, it is placed into a special zombie state until the parent calls wait() or waitpid(). wait4(): enables a process to wait for the termination of a specific process Allocating the Process Descriptor The kernel stores the list of processes in a circular doubly linked list called the task list. Each element in the task list is a process descriptor of the type struct task_struct. The process descriptor (struct task_struct) contains all the information about a specific process. 1 2 3 4 5 6 7 8 9 10 High address +--------------------+ ← start of kernel stack | | | kernel stack | ← function calls, locals | | | ↓ | +--------------------+ | thread_info | which points to task_struct +--------------------+ Low address Manipulating the Current Process State set_task_state(task, state) sets a process’s state. On single-CPU systems, it’s just task-\u0026gt;state = state. On multi-CPU (SMP) systems, it also includes a memory barrier to ensure other CPUs see previous memory writes before the state change. The Process Family Tree All processes are descendants of the init process, whose PID is one. Every process on the system has exactly one parent. Likewise, every process has zero or more children. Copy-on-write Copy-on-write (or COW) is a technique to delay or altogether prevent copying of the data.\nThe duplication of resources occurs only when they are written.\nThe only overhead incurred by fork() is the duplication of the parent’s page tables and the creation of a unique process descriptor for the child.\nvfork() The vfork() system call has the same effect as fork(), except that the page table entries of the parent process are not copied.\nvfork() temporarily allows the child to share the parent’s address space, but the parent’s address space is preserved intact because the parent is suspended and the child either calls exec() (creating a new address space) or _exit().\nKernel Threads A kernel-level process that runs entirely in kernel mode.\nNo user address space (mm_struct = NULL).\nUsed for internal OS tasks like scheduling, memory management, and background work.\nProcess Termination do_exit() kills the process and turns it into a zombie for the parent to collect\nrelease_task() is the final step that frees the task_struct and completely removes the process from the kernel.\nReference Love, Robert. Linux Kernel Development. 3rd ed., Addison-Wesley Professional, 2010. ","date":"2026-01-01T14:49:06+10:30","image":"https://jiajun2001.github.io/p/understand-linux-kernel/Linux_hu_dbd7d28e56d32230.png","permalink":"https://jiajun2001.github.io/p/understand-linux-kernel/","title":"Understand Linux Kernel"},{"content":"\nWhat is a distributed system A system where the software components that make up the system execute on two (or typically more) computers. Key Challenges Heterogeneity Varieties and differences in networks, computer hardware, OS, programming languages, implementations. Address heterogeneity through the use of middleware. Scalability A scalable system operates effectively when there is a significant increase in the number of resources and the number of users. Scalability is a mean for controlling performance loss and avoiding performance bottlenecks. Transparency Concealment from the user and application programmer of the separation of components in a distributed system. Access transparency: local and remote resources can be accessed using identical operations. Location transparency: resources can be accessed without knowledge of their physical or network location. Openness Can the system be extended and re-implemented Degree to which new resources-sharing services can be added and be made available for use by a variety of client programs. Specification and documentation of key interfaces must be published. Open Systems: An open system is one where the components are built to common public standards. Security A variety of points that need to be protected: data flow, network, coordination and allocation service, data management / consistency service. RPC Definition: RPC is a protocol that lets a program call a function on a remote machine as if it were a local function call. Components: Client Stub: Acts like a local function but sends a request over the network Server Stub (Skeleton): Receives the request and invokes the real function Transport: Often uses HTTP, TCP, or HTTP/2 Java RMI A distributed Java program is one which the objects making up the program reside on two or more separate computers (virtual machines). Java Remote Method Invocation (for object oriented programming) provides support for clients invoking methods on remote servers. Write code in C -\u0026gt; RPC (Remote Procedure Call) Connection Server publishes its name and location to the RMI Registry through the RMI URLs Client asks the RMI Registry about the server. Both the RMIRegistry and Server listen on sockets for remote requests. Clients need to identify both the host where the server resides and the socket number (port) that it is listening on. Invocation Server gives its Stub (local proxy) to the RMIRegistry, which gives it to the client. Stub implements the same method signatures as the Server. The Client invokes methods on the Stab as if it were invoking the methods directly on the Server. The Stub is then response for sending the request to the Server VM. The Stab serialises the method invocation across a socket connection connecting the Client and Server. It sends the method name, and the parameters of the method invocation. It waits for a return object or exception objects to be sent back. Synchronous method invocation Server side The server has a local proxy (Skeleton) for the Client. Skeleton creates a socket on which to listen for Client requests. Skeleton receives requests, de-serialises them, and invokes the method on the Server on behalf of the Client. Skeleton sends return result or exception objects back to Stub. An Open Java RMI System The communications protocol used when sending data between the Stub and Skeleton The application specific definition of the methods available for remote method invocation ","date":"2025-08-03T10:27:22+10:30","image":"https://jiajun2001.github.io/p/distributed-systems/picture_hu_18626136d1ac7632.png","permalink":"https://jiajun2001.github.io/p/distributed-systems/","title":"Distributed Systems"},{"content":"In this post, I’ll break down the C++ build pipeline step-by-step — from preprocessing and compilation to linking and execution. Whether you\u0026rsquo;re revisiting the basics or deepening your systems-level knowledge, this guide will help demystify what actually happens when you hit \u0026ldquo;build\u0026rdquo;.\nInitial Program 1 2 3 4 5 6 7 8 9 10 11 12 // square.cpp #define INPUT 5 int square(int n) { return n * n; } // Comment: This code is used for experiment int main() { return square(INPUT); } Step 1: Preprocessing Purpose\nPreprocessor expands macros (#define INPUT 5) It would also process: #include headers, #ifdef, #pragma, etc. Comments are removed Run clang++ -E square.cpp -o square.i\nResult\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // square.i # 1 \u0026#34;square.cpp\u0026#34; # 1 \u0026#34;\u0026lt;built-in\u0026gt;\u0026#34; 1 # 1 \u0026#34;\u0026lt;built-in\u0026gt;\u0026#34; 3 # 384 \u0026#34;\u0026lt;built-in\u0026gt;\u0026#34; 3 # 1 \u0026#34;\u0026lt;command line\u0026gt;\u0026#34; 1 # 1 \u0026#34;\u0026lt;built-in\u0026gt;\u0026#34; 2 # 1 \u0026#34;square.cpp\u0026#34; 2 int square(int n) { return n * n; } int main() { return square(5); } Step 2: Compile to Assembly Purpose:\nCompiler translates C++ code into assembly code (human-readable, low-level instructions). This is architecture-specific (e.g., x86_64 or ARM). Run clang++ -S square.i -o square.s\nResult\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 .section\t__TEXT,__text,regular,pure_instructions .build_version macos, 14, 0\tsdk_version 14, 2 .globl\t__Z6squarei ## -- Begin function _Z6squarei .p2align\t4, 0x90 __Z6squarei: ## @_Z6squarei .cfi_startproc ## %bb.0: pushq\t%rbp .cfi_def_cfa_offset 16 .cfi_offset %rbp, -16 movq\t%rsp, %rbp .cfi_def_cfa_register %rbp movl\t%edi, -4(%rbp) movl\t-4(%rbp), %eax imull\t-4(%rbp), %eax popq\t%rbp retq .cfi_endproc ## -- End function .globl\t_main ## -- Begin function main .p2align\t4, 0x90 _main: ## @main .cfi_startproc ## %bb.0: pushq\t%rbp .cfi_def_cfa_offset 16 .cfi_offset %rbp, -16 movq\t%rsp, %rbp .cfi_def_cfa_register %rbp subq\t$16, %rsp movl\t$0, -4(%rbp) movl\t$5, %edi callq\t__Z6squarei addq\t$16, %rsp popq\t%rbp retq .cfi_endproc ## -- End function .subsections_via_symbols You’ll now have a file square.s, which contains assembly instructions. Step 3: Assemble to Object Code Purpose:\nThe assembler turns the assembly code into binary machine code This is stored in an object file (.o), which can\u0026rsquo;t run on its own Run clang++ -c square.s -o square.o\nDisassemble with objdump -SC square.o \u0026gt; square.lst to see what\u0026rsquo;s going on with square.o since we cannot open a binary file\nResult\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 square.o:\tfile format mach-o 64-bit x86-64 Disassembly of section __TEXT,__text: 0000000000000000 \u0026lt;square(int)\u0026gt;: 0: 55 pushq\t%rbp 1: 48 89 e5 movq\t%rsp, %rbp 4: 89 7d fc movl\t%edi, -4(%rbp) 7: 8b 45 fc movl\t-4(%rbp), %eax a: 0f af 45 fc imull\t-4(%rbp), %eax e: 5d popq\t%rbp f: c3 retq 0000000000000010 \u0026lt;_main\u0026gt;: 10: 55 pushq\t%rbp 11: 48 89 e5 movq\t%rsp, %rbp 14: 48 83 ec 10 subq\t$16, %rsp 18: c7 45 fc 00 00 00 00 movl\t$0, -4(%rbp) 1f: bf 05 00 00 00 movl\t$5, %edi 24: e8 00 00 00 00 callq\t0x29 \u0026lt;_main+0x19\u0026gt; 29: 48 83 c4 10 addq\t$16, %rsp 2d: 5d popq\t%rbp 2e: c3 retq Step 4: Link to Executable Purpose:\nThe linker takes your object file, startup/runtime code (such as _start), and any required standard library functions (e.g., std::cout if used), and links them together into a single executable. Run clang++ square.o -o square\nResult\n1 2 3 (base) apple@Macbook Compilation % ./square echo $? 25 ","date":"2025-07-06T15:25:06+10:30","image":"https://jiajun2001.github.io/p/compile-c-program/picture_hu_f18a85b715016dfa.png","permalink":"https://jiajun2001.github.io/p/compile-c-program/","title":"Compile C++ Program"},{"content":" Parallel Programming Why we need ever-increasing performance As our computational power increases, the number of problems that we can seriously consider solving also increases. E.g. climate modelling / protein folding / drug discovery / energy research / data analysis Why building parallel systems As the speed of transistors increases, their power consumption increases. Most of this power is dissipated as heat, and when an integrated circuit gets too hot, it becomes unreliable. Transistor density can increase for a while, but slower than before. Write parallel program Basic idea: partition the work to be done among cores Two widely used approaches Task-parallelism: partition the various tasks carried out in solving the problem among the cores Each tutor grades one question Data-parallelism: partition data used in solving the problem among the cores, and each core carries out more or less similar operations on its part of the data Each tutor grades one pile of exam paper Coordination of cores Communication: one or more cores send their message to another core Load balancing: make sure the cores all to be assigned roughly the same number of works Synchronization: the cores need to wait before other cores Two types of parallel systems Shared-memory system: we can coordinate the cores by having them examine and update shared-memory locations Distributed-memory system: each core has its own, private memory, and the core must communicate explicitly by doing something like sending messages across a network Concurrent, parallel and distributed In concurrent computing: a program is one in which multiple tasks can be in progress at any instant In parallel computing: a program is one in which multiple tasks cooperate closely to solve a problem In distributed computing: a program may need to cooperate with other programs to solve a problem Parallel Hardware and Software von Neumann Architecture The classical von Neumann architecture consists of main memory, a central processing unit (CPU), and an interconnection between the memory and the CPU. Main memory consists of a collection of locations, each of which is capable of storing both instructions and data. The central processing unit is divided into a control unit and an arithmetic and logic unit (ALU). The control unit is responsible for deciding which instructions in a program should be executed. The control unit has a special register called the program counter which stores the address of the next instruction to be executed. The ALU is responsible for executing the actual instructions. Data in the CPU and information about the state of an executing program are stored in very fast storage called registers. Instructions and data are transferred between the CPU and memory via the interconnect. This has traditionally been a bus, which consists of a collection of parallel wires and some hardware controlling access to the wires. von Neumann bottleneck: The separation of memory and CPU is often called the von Neumann bottleneck, since the interconnect determines the rate at which instructions and data can be accessed. Multitasking: The OS provides support for the apparent simultaneous execution of multiple programs. This is possible even on a system with a single core, since each process runs for a small interval of time (time slice). After one running program has executed for a time slice, the OS can run a different program. Process: When a user runs a program, the OS creates a process which is an instance of a computer program that is being executed. Threading: provides a mechanism for programmers to divide their programs into more or less independent tasks with the property that when one thread is blocked another thread can be run. In addition, it is possible to switch between threads much faster than switching between processes. Thread: Threads are contained within process, so they can use the same executable, and they usually share the same memory and the same I/O devices. The two most important exceptions are that they will need a record of their own PC and call stacks so that they can execute independently of each other. When a thread is started, it folks off the process, when a thread terminates, it joins the process. Flynn\u0026rsquo;s taxonomy: classify computer architectures SISD (single instruction, single data): executes a single instruction at a time and it can fetch or store one item of data at a time. (classical von Neumann system) SIMD (single instruction, multiple data): operates on multiple data streams by applying the same instruction to multiple data items. Vector processors: operates on arrays or vectors of data. Vector systems have very high memory bandwidth, and every data item that is loaded is actually used. However, they do not handle irregular data structure as well as other parallel architectures, and there seems to be a very finite limit to their scalability. Graphic processing units: converts the internal representation into an array of pixels that can be sent to a computer screen. Several of the stages of this pipeline are programmable. The behavior of the programmable stages is specified by functions called shader functions. All GPUs use SIMD parallelism, GPUs are not pure SIMD systems since current generation GPUs can have dozens of cores, which are capable of executing independent instructions streams. MIMD (multiple instruction, multiple data): supports multiple simultaneous instruction streams operating on multiple data streams. Thus, MIMD systems typically consist of a collection of fully independent processing units or cores, each of which has its own control unit and its own ALU (asynchronous). Shared-memory systems (multiple multicore processors) Uniform memory access (UMA): The time to access all memory locations will be the same for all cores / Easier to program Nonuniform memory access (NUMA): Faster access to the directly connected memory / Have the potential to use larger amount of memeory Distributed-memory systems (clusters) Clusters are composed of a collection of commodity systems (PC), or connected by a commodity interconnection network (Ethernet). Difference between SPMD and SIMD SPMD refers to a programming model where a single program is executed by multiple parallel processing units, such as multiple threads or processes. Each processing unit operates on its own portion of the data, but they all execute the same program. SPMD allows for flexible control flow and can be used to express both data parallel and task parallel computations. SIMD refers to a type of parallelism where a single instruction is executed simultaneously by multiple processing elements on different data elements. SIMD architectures typically have a single control unit that broadcasts instructions to multiple processing units, allowing for efficient parallel execution of the same operation on multiple data elements in parallel. Communication and synchronization In distributed-memory programs, we often implicitly synchronize the process by communicating among them. In shared-memory programs, we often communicate among the threads by synchronizing them. Embarrassingly parallel Programs that can be parallelized by simply dividing the work among the processes / threads. Programming with Message Passing using MPI Distributed-memory systems using message-passing Process: a program running on one core-memory pair Message-Passing Interface (MPI): one process calls a send function and the other calls a receive function. Processes typically identify each other by ranks in the range 0, 1, \u0026hellip;, p - 1, where p is the number of processes. MPI functions MPI_Init(): tells MPI system to do all of the necessary setup. It might allocate storage for message buffers, and it might decide which process gets which rank. No other MPI functions should be called before the program calls MPI_Init(). MPI_Finalize(): tells MPI system that we are done using MPI, and that any resources allocated for MPI can be freed. No MPI functions should be called after the call to MPI_Finalize(). MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;comm_size): returns in its second argument the number of processes in the communicator MPI_Comm_rand(MPI_COMM_WORLD, \u0026amp;comm_rank): returns in its second argument the calling process\u0026rsquo;s rank in the communicator MPI_Send(msg_buf_p, msg_size, msg_type, dest, tag, communicator) msg_buf_p: a pointer points to the block memory containing the contents of the message msg_size: amount of data to be sent msg_type: type of data to be sent dest: specifies the rank of the process that should receive the message tag: it can be used to distinguish messages that are otherwise identical communicator: a message sent by a process using one communicator cannot be received by a process that\u0026rsquo;s using a different communicator MPI_Recv(msg_buf_p, buf_size, buf_type, source, tag, communicator, status_p) msg_buf_p: points to the block of memory buf_size: determines the number of objects that can be stored in the block buf_type: indicates the type of the objects source: specifies the process from which the message should be received (can use MPI_ANY_SOURCE) tag: should match the communicator used by the sending process (can use MPI_ANY_TAG) communicator: match the communicator used by the sending process status_p: it is a structure with at least three members MPI_SOURCE, MPI_TAG, MPI_ERROR, after a call to MPI_Recv in which \u0026amp;status is passed as the last argument, we can determine the sender and tag. Semantics of MPI_Send and MPI_Recv The sending process will assemble the message If the sending process buffers the message, the MPI system will place the message into its own internal storage If the system blocks, it will wait until it can begin transmitting the message, and the call to MPI_Send may not return immediately MPI_Recv always blocks until a matching message has been received MPI requires that messages be non-overtaking. This means that if process q sends two messages to process r, then the first message sent by q must be available to r before the second message. However, there is no restriction on the arrival of messages sent from different processors. Blocking and Non-blocking sends and receives Blocking Send is complete when the message buffer has been fully transferred to the MPI system. Receive is complete when the message data has arrived at the destination and is available for use. Non-blocking They just continue with no regards for completion status Can be useful to help avoid deadlock Commands for compile and run 1 2 mpicc -g -Wall -o mpi_hello mpi_hello.c mpiexec -n 4 ./mpi_hello Collective operations MPI_Bcast(): Copies data from root node to the same memory location in every other node MPI_Gather(): Each node sends the contents of the send buffer to the root node, and root node stores them in rank order. MPI_Scatter(): Root process splits buffer into equal chunks and sends one chunk to each processor MPI_AlltoAll(): Each node performs a Scatter operation on its own data. Thus every node receives some data from every other node. MPI_Reduce(): MPI_Reduce operation combines the values from all processes and produces a single result, which is typically stored on a designated root process. Output Most MPI implementation allow all processes to execute printf and fprintf(stderr, ...). However, most MPI implementations don\u0026rsquo;t provide any automatic scheduling of access to these devices, since MPI processes are competing for access to the shared output devices, stdout, and it is impossible to predict the order in which the processes\u0026rsquo;s output will be queued up. Hence, we can have each process other than 0 send its output to process 0, and process 0 can print the output in process rank order. Input Most MPI implementations only allow process 0 in MPI_COMM_WORLD access to stdin. In order to write MPI programs that can use scanf, we need to branch on process rank, with process 0 reading in the data and then sending it to the other processes. Interconnection Networks Shared-memory interconnects Bus: a collection of parallel communication wires together with some hardware that controls access to the bus. The key characteristic of a bus is that the communication wires are shared by the devices that are connected to it. However, as the number of devices connected to the bus increases, the likelihood that there will be contention for use of the bus increases, and the expected performance of the bus decreases. Crossbars: switched interconnects use switches to control the routing of data among connected devices. Crossbar allows simultaneous communication among different devices, so they are much faster than buses. However, the cost of the switches and links is relatively high. Distributed-memory interconnects Interconnects Direct interconnects: each switch is directly connected to a processor-memory pair, and the switches are connected to each other. Ring: If there are p processors, the number of links is 2p. Toroidal mesh: If there are p processors, the number of links is 3p. Fully connected network: It is used as a basis for evaluating other interconnects. However, it is impractical since it requires a total of p2 / 2 + p / 2 links, and each switch must be capable of connecting to p links. Hypercube: A hypercube of dimension d has a p = 2d nodes, and a switch in a d-dimensional hypercube is directly connected to a processor and d switches. Indirect interconnects: the switches may not be directly connected to a processor. They are often shown with unidirectional links and a collection of processors, each of which has an outgoing and an incoming link, and a switching network. Crossbar: As long as two processors do not attempt to communicate with the same processor, all processors can simultaneously communicate with another processor. (Crossbar \u0026gt; MIN \u0026gt; Bus) omega network: There are communications that cannot occur simultaneously. Bisection width It refers to a measure of the communication capacity or bandwidth between two halves of a system when it is divided into two equal parts. The bisection width is a metric that quantifies the communication capacity between these two groups. It represents the maximum amount of data that can be exchanged between the two halves of the system simultaneously. An alternative way of computing the bisection width is to remove the minimum number of links needed to split the set of nodes into two equal halves. The number of links removed is the bisection width. Bisection bandwidth often used as a measure of network quality it sums the bandwidth of links Latency: the time that elapses between the source\u0026rsquo;s beginning to transmit the data and the destination\u0026rsquo;s starting to receive the first byte. Bandwidth: the rate at which the destination receives data after it has started to receive the first byte Time to transmit a message of n bytes message transmission time = latency + n / bandwidth Parallel Program Design Foster\u0026rsquo;s methodology Partitioning Divide the computation to be performed and the data operated on by the computation into small tasks. The focus here should be on identifying tasks that can be executed in parallel. Communication Determine what communication needs to be carried out among the tasks identified in the previous step. Agglomeration or aggregation Combine tasks and communications identified in the first step into larger tasks. For example, if task A must be executed before task B can be executed, it may make sense to aggregate them into a single composite task. Mapping Assign the composite tasks identified in the previous step to processors / threads. This should be done so that communication is minimized, and each process / thread gets roughly the same amount of work. Performance Analysis Linear speedup: If we call the serial run-time Tserial and our parallel run-time Tparallel, then the best we can hope for is Tparallel = Tserial / p. However, in practice, we are unlikely to get linear speedup becuase the use of multiple processes / threads almost invariably introduces some overhead. Speedup of a parallel program Speedup refers to the performance improvement achieved by executing a program on multiple processors compared to running it on a single processor. S = Tserial / Tparallel Efficiency of a parallel program E = S / p = (Tserial / Tparallel) / p Effect of problem size When we increase the problem size, the speedups and the efficiencies increase, while they decrease when we decrease the problem size (when p is not small). The relationship between problem size and speedup depends on various factors, including the nature of the problem, the parallelization technique used, the hardware architecture, and the efficiency of the parallel algorithm. Since Tparallel = (Tserial / p) + Toverhead, if there\u0026rsquo;s more work for the processes / threads to do, the relative amount of time spent coordinating the work of the processes / threads should be less. Amdahl\u0026rsquo;s law Roughly, that unless virtually all of a serial program is parallelized, the possible speedup is going to be very limited - regardless of the number of cores available. More generally, if a fraction r of our serial program remains unparallelized, then Amdahl\u0026rsquo;s law says we cannot get a speedup better than 1/r. Gustafson\u0026rsquo;s law For many problems, as we increase the problem size, the \u0026lsquo;inherently\u0026rsquo; serial fraction of the problem decreases in size. (No need to worry about Amdahl\u0026rsquo;s law) Scalability Suppose we now increase the number of processes / threads that are used by the program. If we can find a corresponding rate of increase in the problem size so that the program always has efficiency E, then the program is scalable. Types of scalable cases Strongly scalable: if when we increase the number of processes / threads, we can keep the efficiency fixed without increasing the problem size. Weakly scalable: if we can keep the efficiency fixed by increasing the problem size at the same rate as we increase the number of processes / threads. Taking timings Resolution: the unit of measurement on the timer. It is the duration of the shortest event that can have a nonzero time. We first execute a barrier function that approximately synchronizes all of the processes / threads. We then execute the code and each process / thread finds the time it took. Then all the processes / threads call a global maximum function, which returns the largest of the elapsed times, and process / thread 0 prints it out. If we run the same experiment several times, we usually report the minimum time. We rarely run more than one thread per core. We usually not include I/O in our reported run-times. Barrier A barrier is a mechanism that imposes global synchronization, ensuring that all participating threads or processes reach a specific point before continuing, thereby enabling coordination and synchronization. Multithreading Programming Dynamic and static threads Dynamic thread There is often a master thread and at any given instant a (possibly empty) collection of worker threads. The master thread typically waits for work requests, and when a new request arrives, it folks a worker thread, the thread carries out the request, and when the thread completes the work, it terminates and joins the master thread. Static thread In this paradigm, all of the threads are forked after any needed setup by the master thread and the threads run until all the work is completed. The static thread paradigm has the potential for better performance than the dynamic paradigm. Static thread paradigm is often used. Nondeterminism A computation is nondeterministic if a given input can result in different outputs. Race condition When threads or processes attempt to simultaneously access a resource, and the accesses can result in an error. Critical section A block of code that can only be executed by one thread at a time. Mutual exclusion lock / Mutex / Lock The most commonly used mechanism for insuring mutual exclusion Before a thread can execute the code in the critical section, it must \u0026lsquo;obtain\u0026rsquo; the mutex by calling a mutex function, and when it finishes executing the code in the critical section, it should relinquish the mutex by calling an unlock function. While one thread owns the lock, any other thread attempting to execute the code in the critical section will wait in its call to the lock function. Busy-waiting A thread enters a loop whose sole purpose is to test a condition. However, it can be very wasteful of system resources. Thread safety A static variable that is declared in a function persists from one call to the next. Hence, static variables are effectively shared among any threads that call the function, and this can have unexpected and unwanted consequences. Deadlock Deadlock refers to a situation in a concurrent system where two or more processes or threads are unable to proceed because each is waiting for a resource that is held by another process or thread in the system. Programming with OpenMP OpenMP provides what\u0026rsquo;s known as a \u0026ldquo;directive-based\u0026rdquo; shared-memory API. In C and C++, this means that there are special preprocessor instructions known as pragmas. Pragmas are typically added to a system to allow behaviors that are not part of the basic C specification. Compilers that do not support the pragmas are free to ignore them. So, in principle, if you have a carefully written OpenMP program, it can be compiled and run on any system with a C compiler, regardless of whether the compiler supports OpenMP. Commands for compile and run 1 2 gcc -g -Wall -fopenmp -o omp_hello omp_hello.c ./omp_hello 4 Parallel for The parallel for directive forks a team of threads to execute the following structured block. However, the structured block following the parallel for directive must be a for loop. The system parallelizes the for loop by dividing the iterations of the loop among threads. Loop-carried dependence A loop in which the results of one or more iterations depend on other iterations cannot, in general, be correctly parallelized by OpenMP. Reduction clause OpenMP creates a private variable for each thread, and the run-time system stores each thread’s result in this private variable. OpenMP also creates a critical section and the values stored in the private variables are added in this critical section. Syntax: reduction(\u0026lt;operator\u0026gt;: \u0026lt;variable list\u0026gt;) Scope of variables A variable that can be accessed by all the threads in the team has shared scope, while a variable that can only be accessed by a single thread has private scope. Performance Issues and Optimisation Four Key Challenges Synchronization: Threads often compete for shared data — protecting it (with locks/barriers) adds overhead. Load Balancing: Work isn’t always evenly divisible — some threads finish early while others lag behind. Locality \u0026amp; Cache Reuse: If your memory access pattern is poor, you lose time waiting on RAM instead of computing. Granularity: In parallel computing, we often refer it to be the amount of work divided as sub-tasks. If it is too fine-grained, meaning there are quite a lot of sub-tasks while each of them is small. Managing them can lead to big overhead. If it is too coarse-grained, meaning there are a small number of tasks while each of them is huge. Some cores will finish tasks earlier than others, so they will be under-utilized. OpenMP Scheduling Block Partitioning (static scheduling) Divide data or tasks into big chunks consecutively, each PE is responsible for a block of data or tasks Pros and cons: Cache-friendly, good for computationally intensive tasks Might lead to load imbalance if some blocks are computationally-heavy Cyclic Partitioning (static scheduling) Assign tasks in round-robin fashion, like dealing cards Pros and cons: Better for load balance especially task workload and data are not evenly distributed Cache hit rate might be low Dynamic Scheduling Threads request chunks at runtime, use when iteration is unpredictable Pros and cons: Best load balance High overhead in scheduling Guided Scheduling Similar to dynamic scheduling, but will shrink chunks later, use when we need to balance scheduling cost and load balance Pros and cons: Lower scheduling cost comparing with dynamic scheduling Better load balance comparing with static scheduling High performance Balance workload onto available execution resources Identify and manage concurrency Distributed Schedulers in clusters User submit their jobs to a cluster\u0026rsquo;s scheduler Jobs in queue considered for allocation whenever state of a machine changes (event-driven) Reduce communications Decomposing a problem into multiple tasks usually means there will communication among tasks Reduce extra work (overhead) performed to increase parallelism, manage assignment Communication to Computation Ratio amount of communication / amount of computation Amount of low ratio to effectively utilize modern parallel processors Communication Inherent communication Information that fundamentally must be moved between processors to carry out the algorithm given the specified assignment Artifactual communication All other communication Contention Contention occurs when many requests to a resource are made within a small window of time Improving program performance Identify and exploit locality Reduce overhead Reduce contention Maximize overlap of communication and processing Batch scheduling Batch scheduling is a method of managing and executing computational tasks onhigh-performance computers in a non-interactive manner. In batch scheduling,users submit their jobs or tasks to a job scheduler, which allocates computingresources to the execution of these jobs based on various policies and priorities. Advantages: efficient resource utilization, fairness, workload managementcapabilities, and system stability Parallel Hardware: GPUs and NPUs Definition of GPGPUs GPGPU is the use of a GPU (Graphics Processing Unit) — which is traditionally used for rendering graphics — to perform non-graphics, general-purpose computations that are typically handled by the CPU (Central Processing Unit) GPUs are not always well suited to caching The data access patterns of graphics applications make it difficult for them to fully utilize traditional caching mechanisms, because their computation results do not need to be frequently written back to memory, nor are intermediate results suitable for long-term retention. This leads to low cache hit rates, which can actually reduce processing efficiency. OpenCL Definition OpenCL (Open Computing Language) is an open standard and framework for programming heterogeneous computing platforms, including CPUs, GPUs, and other accelerators. It provides a unified programming model and API (Application Programming Interface) that allows developers to write code that can run efficiently across different hardware architectures. OpenCL memory model The OpenCL memory model provides a framework for managing different types of memory in OpenCL computations, including private, local, global, and constant memory. It establishes rules for data organization, movement, and synchronization, allowing efficient utilization of memory resources and proper synchronization in parallel computations. OpenCL execution model Application runs on a host which submits work to devices Work item: the basic unit of work on an OpenCL device Kernel: the code for a work item (basically a C function) Program: collection of kernels and other functions Context The environment within which work-items execute. It includes devices and their memories and command queues. Command queue: a queue used by the host application to submit work to a device Work is queued in-order, one queue per device Work can be executed in-order or out-of-order Steps that a host program must include, when using OpenCL for parallel work. Platform discovery / Device selection / context creation / Command queue creation / Program compilation / Kernel creation / Memory object creation and data-transfer / Kernel execution / Synchronization and result retrieval / Cleanup Restrictions Limited Functionality of Standard C Library OpenCL restricts the functionality of the standard C library that is available for use within kernel code. The reason is to maintain portability across diverse hardware platforms. Different OpenCL devices may have varying capabilities and may lack support for certain standard C library functions. Limited Data Type OpenCL restricts the available data types in kernel code compared to standard C. More complex data types, such as structures and unions, are not directly supported in kernel code. This restriction aims to facilitate efficient memory access and vectorization in parallel computations. ","date":"2025-05-18T15:27:22+10:30","image":"https://jiajun2001.github.io/p/parallel-and-distributed-computing/background_hu_dbbdaa2959bf9c1d.jpeg","permalink":"https://jiajun2001.github.io/p/parallel-and-distributed-computing/","title":"Parallel and Distributed Computing"},{"content":"During the summer of my freshman year, I spent quite a lot of time self-studying C++ basic knowledge such as operator overloading, template, file operations. Honestly, it is sufficient for me to pass my courses in later 2 years. However, when I am coding for real industrial project, I realized the knowledge gap. That\u0026rsquo;s why I make this post to keep track of my further study about C++. In this post, I will go through some deeper concepts in C++ such as compiling, linking, some other keywords, smart pointers, C++ libraries, multi threading, design patters, lambda expressions\u0026hellip;\nIntroduction to C++ C++ is a cross-platform language that can be used to create high-performance applications. Operating System, Game Engines, Browsers\u0026hellip; C++ was developed as an extension to the C language and C++ supports classes and objects. C++ gives programmers a high level of control over system resources and memory. C++ Variable Size On 32-bit machine the size of a pointer is 32 bits (4 bytes), while on 64 bit machine it\u0026rsquo;s 8 bytes. Regardless of what data type they are pointing to, they have fixed size. Using Libraries Static linking: the library will be put in the executable file at compile time Technically faster, since the compiler and linker can perform linking optimization Dynamic linking: the library gets linked at run-time Stack and Heap Stack is a typically an area of memory that has predefined size, usually around 2 MB 1 int value = 999; 1 movl\t$999, -4(%rbp) Heap can grow and change as application goes on 1 int* value = new int(999); // way more complex and expensive operation 1 2 3 4 5 callq\t__Znwm movq\t%rax, %rcx movq\t%rcx, %rax movl\t$999, (%rcx) ## imm = 0x3E7 movq\t%rax, -8(%rbp) Constant Keyword Constant pointer and pointer constant 1 2 3 4 5 6 7 8 9 // Constant Pointer const int* constPtr = new int(99); // *constPtr = 100; // Error: cannot change the content that the pointer is pointing to constPtr = new int(100); // Can change where it points to // Pointer Constant int* const ptrConst = new int(99); // ptrConst = new int(100); // Error: cannot change where it points to *ptrConst = 100; // Can change the content that the pointer is pointing to Constant method in a class 1 2 3 4 5 6 7 8 9 10 11 class Entity { private: int val; mutable int somethingHasToChange; // \u0026#34;mutable\u0026#34; allows us to change the value in a const method public: int getVal() const { // val++; // Error: cannot modify the actual class somethingHasToChange++; return val; } }; Explicit Keyword 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Entity { private: string m_Name; int m_Age; public: Entity(const string\u0026amp; name) : m_Name(name), m_Age(-1) {} // \u0026#34;explicit\u0026#34; keyword disables implicit casting // \u0026#34;Entitiy b = 22\u0026#34; is not allowed anymore explicit Entity(int age) : m_Name(\u0026#34;unknown\u0026#34;), m_Age(age) {} }; void printEntity(const Entity\u0026amp; entity) {} int main(void) { Entity a = string(\u0026#34;Jason\u0026#34;); Entity b = (Entity)22; printEntity(string(\u0026#34;Jason\u0026#34;)); printEntity(Entity(22)); } Operator Overloading 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 struct Vector2 { float x, y; Vector2(float x, float y) : x(x), y(y) {} Vector2 operator+(const Vector2\u0026amp; other) const { return Vector2(x + other.x, y + other.y); } Vector2 operator*(const Vector2\u0026amp; other) const { return Vector2(x * other.x, y * other.y); } bool operator==(const Vector2\u0026amp; other) const { return x == other.x \u0026amp;\u0026amp; y == other.y; } bool operator!=(const Vector2\u0026amp; other) const { return !(*this == other); } }; ostream\u0026amp; operator\u0026lt;\u0026lt;(ostream\u0026amp; stream, const Vector2\u0026amp; vec2) { stream \u0026lt;\u0026lt; \u0026#34;The data of vec2 is \u0026#34;\u0026lt;\u0026lt; vec2.x \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; vec2.y \u0026lt;\u0026lt; \u0026#34;.\u0026#34;; return stream; } Smart Pointers unique pointer (has to be unique) 1 2 3 4 { unique_ptr\u0026lt;Entity\u0026gt; entity = make_unique\u0026lt;Entity\u0026gt;(); entity-\u0026gt;Print(); } shared pointer (has reference count) 1 2 3 4 5 6 7 { shared_ptr\u0026lt;Entity\u0026gt; e0; { shared_ptr\u0026lt;Entity\u0026gt; sharedEntity = make_shared\u0026lt;Entity\u0026gt;(); e0 = sharedEntity; } } // Entity object gets de-allocated after here weak pointer (does not increase the reference count) 1 2 3 4 5 6 7 { weak_ptr\u0026lt;Entity\u0026gt; e0; { shared_ptr\u0026lt;Entity\u0026gt; sharedEntity = make_shared\u0026lt;Entity\u0026gt;(); e0 = sharedEntity; } // Entity object gets de-allocated after here } Shallow Copy and Deep Copy Shallow copy 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class String { private: char* m_Buffer; unsigned int m_Size; public: String(const char* string) { m_Size = strlen(string); m_Buffer = new char[m_Size + 1]; memcpy(m_Buffer, string, m_Size); m_Buffer[m_Size] = \u0026#39;\\0\u0026#39;; } ~String() { std::cout \u0026lt;\u0026lt; \u0026#34;Attempting to destruct \u0026#34; \u0026lt;\u0026lt; (void*)(this-\u0026gt;m_Buffer) \u0026lt;\u0026lt; std::endl; delete[] m_Buffer; } friend std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; stream, const String\u0026amp; stringObject); }; std::ostream\u0026amp; operator\u0026lt;\u0026lt;(std::ostream\u0026amp; stream, const String\u0026amp; stringObject) { stream \u0026lt;\u0026lt; stringObject.m_Buffer; return stream; } int main(void) { String stringObject(\u0026#34;Jason\u0026#34;); std::cout \u0026lt;\u0026lt; stringObject \u0026lt;\u0026lt; std::endl; String second = stringObject; std::cout \u0026lt;\u0026lt; second \u0026lt;\u0026lt; std::endl; /* Jason Jason Attempting to destruct 0x7fc833f05a60 Attempting to destruct 0x7fc833f05a60 demo(64695,0x7ff8547fd4c0) malloc: Double free of object 0x7fc833f05a60 demo(64695,0x7ff8547fd4c0) malloc: *** set a breakpoint in malloc_error_break to debug zsh: abort ./demo */ return 0; } This issue is caused by both \u0026lsquo;String\u0026rsquo; objects having the same \u0026lsquo;char* m_Buffer\u0026rsquo; variable and value so that this variable will be freed twice. Deep copy 1 2 3 4 String(const String\u0026amp; other) : m_Size(other.m_Size) { m_Buffer = new char[m_Size + 1]; memcpy(m_Buffer, other.m_Buffer, m_Size + 1); } Array An array is a collection of elements of the same type placed in contiguous memory locations that can be individually referenced by using an index to a unique identifier. Loop through an array 1 2 3 4 5 6 7 8 9 10 11 // Using index int myNumbers[5] = {10, 20, 30, 40, 50}; for (int i = 0; i \u0026lt; sizeof(myNumbers) / sizeof(int); i++) { cout \u0026lt;\u0026lt; myNumbers[i] \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } // Enhanced for loop int myNumbers[5] = {10, 20, 30, 40, 50}; for (int i : myNumbers) { cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; } STL Array Has boundary checking / can return size / stored on the stack Loop through an array 1 2 3 4 5 6 7 8 9 10 11 12 13 14 template\u0026lt;typename T\u0026gt; void PrintArray(const T\u0026amp; data) { for (int i = 0; i \u0026lt; data.size(); i++) { std::cout \u0026lt;\u0026lt; data[i] \u0026lt;\u0026lt; std::endl; } } int main(void) { std::array\u0026lt;int, 3\u0026gt; data; data[0] = 0; data[1] = 1; data[2] = 2; PrintArray(data); } String String Concatenation: string fullName = firstName + lastName; string fullName = firstName.append(lastName); String Size: int size = txt.size(); C++ Math Max: cout \u0026laquo; max(5, 10); Min: cout \u0026laquo; min(5, 10); C++ Switch Statement Example code: 1 2 3 4 5 6 7 8 9 10 switch(expression) { case x: // code block break; case y: // code block break; default: // code block } Structure 1 2 3 4 struct { // Structure declaration int myNum; // Member (int variable) string myString; // Member (string variable) } myStructure; // Structure variable Can be used to deal with multiple return values of a function STL Vector Method: vec.size(), vec.empty(), vec.push_back(ele), vec.pop_back(), vec.front(), vec.back() Reduce copy times to improve performance 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 struct Vector2 { float x, y; Vector2(float x, float y) : x(x), y(y) {} Vector2(const Vector2\u0026amp; vec) : x(vec.x), y(vec.y) { std::cout \u0026lt;\u0026lt; \u0026#34;Copied!\u0026#34; \u0026lt;\u0026lt; std::endl; } }; int main(void) { std::vector\u0026lt;Vector2\u0026gt; vector; vector.reserve(3); vector.emplace_back(1, 2); vector.emplace_back(3, 4); vector.emplace_back(5, 6); } STL Set Method: set.insert(ele), s.erase(ele / iterator), s.count(ele), s.find(ele) STL Map Method: map.insert(pair\u0026lt;int,int\u0026gt; (1,10)), map.insert(make_pair(2,200)), map.count(3), map.find(3) STL Pair Initialize pair\u0026lt;string,int\u0026gt; p(\u0026ldquo;Jason\u0026rdquo;,20); pair\u0026lt;string,int\u0026gt; p2 = make_pair(\u0026ldquo;Larry\u0026rdquo;,16); STL List Implemented by a doubly linked list Insertion and deletion operations are fast Access elements is slower that array and takes more space STL Deque Double Ended Queue: Inserting at head is faster than vector Provide random access STL Stack Methods: s.push(ele), s.pop(), s.size(), s.empty(), s.top() STL Queue Methods: q.push(ele), q.pop(), q.size(), q.empty(), q.front(), q.back() Template Using template for a function 1 2 3 4 template\u0026lt;typename T\u0026gt; void Print(T value) { std::cout \u0026lt;\u0026lt; \u0026#34;I am going to print: \u0026#34; \u0026lt;\u0026lt; value \u0026lt;\u0026lt; std::endl; } Using template for a class 1 2 3 4 5 6 7 8 9 template\u0026lt;typename T, int N\u0026gt; class Array { private: T m_array[N]; public: int GetSize() const { return N; } }; Macro 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #if 1 #define DEBUG 1 #if DEBUG == 1 #define LOG(x) std::cout \u0026lt;\u0026lt; x \u0026lt;\u0026lt; std::endl #else #define LOG(x) #endif #endif #define MAIN int main(void) \\ { \\ LOG(\u0026#34;HELLO\u0026#34;); \\ std::cin.get(); \\ } MAIN Function Pointers 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 void HelloWorld(int a) { std::cout \u0026lt;\u0026lt; \u0026#34;Hello World! Value: \u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; } int main(void) { // Example 1 auto funPtr1 = HelloWorld; // Example 2 void (*funPtr2)(int a) = HelloWorld; // Example 3 typedef void (*HelloWorldFunction)(int a); HelloWorldFunction funPtr3 = HelloWorld; funPtr1(99); funPtr2(99); funPtr3(99); } Lambda Expression Lambda is a way for us to define anonymous functions or disposable functions Reference: https://en.cppreference.com/w/cpp/language/lambda 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #include \u0026lt;functional\u0026gt; void ForEach(const std::vector\u0026lt;int\u0026gt;\u0026amp; values, const std::function\u0026lt;void(int)\u0026gt;\u0026amp; funcPtr) { for (int value : values) funcPtr(value); } int main(void) { std::vector\u0026lt;int\u0026gt; values = {1, 2, 3, 4, 5}; int a = 5; std::function\u0026lt;void(int)\u0026gt; lambda = [=](int value) mutable { std::cout \u0026lt;\u0026lt; \u0026#34;Value: \u0026#34; \u0026lt;\u0026lt; value \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;a:\u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; a = 1; }; ForEach(values, lambda); } Namespace The primary purpose of namespace is to avoid naming conflicts. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 namespace apple { void Print(const std::string\u0026amp; text) { std::cout \u0026lt;\u0026lt; text \u0026lt;\u0026lt; std::endl; } } namespace orange { void Print(const char* text) { std::string temp = text; std::reverse(temp.begin(), temp.end()); std::cout \u0026lt;\u0026lt; temp \u0026lt;\u0026lt; std::endl; } } int main(void) { using namespace apple; using namespace orange; // If I want to invoke apple::Print() Print(\u0026#34;Hello\u0026#34;); // silent runtime error: olleH return 0; } Threads 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 #include \u0026lt;iostream\u0026gt; #include \u0026lt;thread\u0026gt; static bool s_Finished = false; void DoWork() { using namespace std::literals::chrono_literals; std::cout \u0026lt;\u0026lt; \u0026#34;Started thread id: \u0026#34; \u0026lt;\u0026lt; std::this_thread::get_id() \u0026lt;\u0026lt; std::endl; while (!s_Finished) { std::cout \u0026lt;\u0026lt; \u0026#34;Working...\u0026#34; \u0026lt;\u0026lt; std::endl; std::this_thread::sleep_for(1s); } std::cout \u0026lt;\u0026lt; \u0026#34;Stop Working...\u0026#34; \u0026lt;\u0026lt; std::endl; } int main(void) { std::thread worker(DoWork); std::cin.get(); s_Finished = true; worker.join(); std::cout \u0026lt;\u0026lt; \u0026#34;Finished\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Started thread id: \u0026#34; \u0026lt;\u0026lt; std::this_thread::get_id() \u0026lt;\u0026lt; std::endl; std::cin.get(); return 0; } /* Started thread id: 0x70000751b000 Working... Working... Working... Working... Stop Working... Finished Started thread id: 0x7ff8547fd4c0 */ Timing 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #include \u0026lt;iostream\u0026gt; #include \u0026lt;chrono\u0026gt; #include \u0026lt;thread\u0026gt; struct Timer { std::chrono::steady_clock::time_point start, end; std::chrono::duration\u0026lt;float\u0026gt; duration; Timer() { start = std::chrono::high_resolution_clock::now(); } ~Timer() { end = std::chrono::high_resolution_clock::now(); duration = end - start; float ms = duration.count() * 1000.0f; std::cout \u0026lt;\u0026lt; \u0026#34;Duration: \u0026#34; \u0026lt;\u0026lt; ms \u0026lt;\u0026lt; \u0026#34;ms\u0026#34; \u0026lt;\u0026lt; std::endl; } }; void function() { Timer timer; for (int i = 0; i \u0026lt; 100; i++) { std::cout \u0026lt;\u0026lt; \u0026#34;Hello\u0026#34; \u0026lt;\u0026lt; std::endl; } } int main(void) { function(); return 0; } Multi-dimensional Arrays 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // 2D array int** array2D = new int*[5]; for (int i = 0; i \u0026lt; 5; i++) { array2D[i] = new int[5]; } // Deallocate a 2D array for (int i = 0; i \u0026lt; 5; i++) { delete[] array2D[i]; } delete[] array2D; // An alternative way (less cache miss, better performance) int* array = new int[5 * 5]; for (int i = 0; i \u0026lt; 5; i++) { for (int j = 0; j \u0026lt; 5; j++) { array[5 * i + j] = 2; } } // 3D Array int*** array3D = new int**[50]; for (int i = 0; i \u0026lt; 50; i++) { array3D[i] = new int*[50]; for (int j = 0; j \u0026lt; 50; j++) { int** ptr = array3D[i]; ptr[j] = new int[50]; } } STL Sorting 1 2 3 std::vector\u0026lt;int\u0026gt; values = {3, 5, 2, 7, 1}; // Comparison function returns true of a is less than b std::sort(values.begin(), values.end(), [](int a, int b){ return a \u0026lt; b; }); Type Punning Type punning is a programming technique that subverts or circumvents the type system of a programming language in order to achieve an effect that would be difficult or impossible to achieve within the bounds of the formal language. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 struct Entity { int x, y; int* getPositions() { return \u0026amp;x; } }; int main(void) { int a = 50; double\u0026amp; value = *(double*)\u0026amp;a; std::cout \u0026lt;\u0026lt; value \u0026lt;\u0026lt; std::endl; // -1.94456e-66 Entity e = {5, 8}; int* position = (int*)\u0026amp;e; int y = *(int*)((char*)\u0026amp;e + 4); std::cout \u0026lt;\u0026lt; position[0] \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; y \u0026lt;\u0026lt; std::endl; } Union A union is a user-defined type in which all members share the same memory location. Type Punning 1 2 3 4 5 6 7 8 9 10 struct Union { union { float a; int b; }; }; Union u; u.a = 2.0f; std::cout \u0026lt;\u0026lt; u.a \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; u.b \u0026lt;\u0026lt; std::endl; // Type Punning: 2, 1073741824 Using multiple ways to address the same data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 struct Vector2 { float x, y; }; struct Vector4 { union { struct { float x, y, z, w; }; struct { Vector2 a, b; // So we technically can view Vector4 as two Vector2 s // Vector2 a shares the same memory address with float x, and y // Vector2 b shares the same memory address with float z, and w }; }; }; void PrintVector2(const Vector2\u0026amp; vector) { std::cout \u0026lt;\u0026lt; vector.x \u0026lt;\u0026lt; \u0026#34;, \u0026#34; \u0026lt;\u0026lt; vector.y \u0026lt;\u0026lt; std::endl; } int main(void) { Vector4 vector = {1.0f, 2.0f, 3.0f, 4.0f}; PrintVector2(vector.a); // 1, 2 vector.z = 500.0f; // 500, 4 PrintVector2(vector.b); } Virtual Destructor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class Base { public: Base() { std::cout \u0026lt;\u0026lt; \u0026#34;Base Constructor\\n\u0026#34;; } // Must put virtual keyword to safely extent the class virtual ~Base() { std::cout \u0026lt;\u0026lt; \u0026#34;Base Destructor\\n\u0026#34;; } }; class Derived : public Base { public: Derived() { std::cout \u0026lt;\u0026lt; \u0026#34;Derived Constructor\\n\u0026#34;; } ~Derived() { std::cout \u0026lt;\u0026lt; \u0026#34;Derived Destructor\\n\u0026#34;; } }; int main(void) { Base* poly = new Derived(); delete poly; // Needs to call the destructor of the derived class if that is present. /* Base Constructor Derived Constructor Derived Destructor Base Destructor */ } Casting 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // ========== static_cast: C++ style double value = 5.25; double s = static_cast\u0026lt;int\u0026gt;(value) + 5.3; // 5 + 5.3 = 10.3 // ========== dynamic_cast: runtime type checking // Instance stores RTTI (runtime type information) Derived* derived = new Derived(); Base* base = derived; // Assume we do not know the type of base AnotherClass* ac = dynamic_cast\u0026lt;AnotherClass*\u0026gt;(base); if (ac) { std::cout \u0026lt;\u0026lt; \u0026#34;base is an instance of AnotherClass\\n\u0026#34;; } else { std::cout \u0026lt;\u0026lt; \u0026#34;base is not an instance of AnotherClass\\n\u0026#34;; } // cout: base is not an instance of AnotherClass // ========== reinterpret_cast: type punning double* r = reinterpret_cast\u0026lt;double*\u0026gt;(\u0026amp;base); // ========== const_cast: add or remove const modifier const int num = 50; const int* ptr1 = \u0026amp;num; std::cout \u0026lt;\u0026lt; \u0026#34;Before: *ptr1 is \u0026#34; \u0026lt;\u0026lt; *ptr1 \u0026lt;\u0026lt; std::endl; // cout: Before: *ptr1 is 50 int* ptr2 = const_cast\u0026lt;int*\u0026gt;(ptr1); *ptr2 = 60; std::cout \u0026lt;\u0026lt; \u0026#34;Now: *ptr1 is \u0026#34; \u0026lt;\u0026lt; *ptr1 \u0026lt;\u0026lt; std::endl; // cout: Now: *ptr1 is 60 Pre-compiled Headers You can precompile both C and C++ programs. In C++ programming, it\u0026rsquo;s common practice to separate class interface information into header files. These header files can later be included in programs that use the class. By precompiling these headers, you can reduce the time a program takes to compile. 1 time g++ pch.h -std=c++11 Benchmark Test 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 #include \u0026lt;iostream\u0026gt; #include \u0026lt;memory\u0026gt; #include \u0026lt;chrono\u0026gt; #include \u0026lt;array\u0026gt; struct Timer { public: Timer() { startTimePoint = std::chrono::high_resolution_clock::now(); } ~Timer() { Stop(); } void Stop() { auto endTimePoint = std::chrono::high_resolution_clock::now(); auto start = std::chrono::time_point_cast\u0026lt;std::chrono::microseconds\u0026gt;(startTimePoint).time_since_epoch().count(); auto end = std::chrono::time_point_cast\u0026lt;std::chrono::microseconds\u0026gt;(endTimePoint).time_since_epoch().count(); auto duration = end - start; double milliseconds = duration * 0.001; // 1 millisecond = 1000 microseconds std::cout \u0026lt;\u0026lt; duration \u0026lt;\u0026lt; \u0026#34; us (\u0026#34; \u0026lt;\u0026lt; milliseconds \u0026lt;\u0026lt; \u0026#34; ms)\\n\u0026#34;; } private: std::chrono::time_point\u0026lt;std::chrono::high_resolution_clock\u0026gt; startTimePoint; }; int main(void) { struct Vector2 { float x, y; }; std::cout \u0026lt;\u0026lt; \u0026#34;Make shared\\n\u0026#34;; { std::array\u0026lt;std::shared_ptr\u0026lt;Vector2\u0026gt;, 1000\u0026gt; sharedPtrs; Timer timer; for (int i = 0; i \u0026lt; sharedPtrs.size(); i++) { sharedPtrs[i] = std::make_shared\u0026lt;Vector2\u0026gt;(); } } std::cout \u0026lt;\u0026lt; \u0026#34;New shared\\n\u0026#34;; { std::array\u0026lt;std::shared_ptr\u0026lt;Vector2\u0026gt;, 1000\u0026gt; sharedPtrs; Timer timer; for (int i = 0; i \u0026lt; sharedPtrs.size(); i++) { sharedPtrs[i] = std::shared_ptr\u0026lt;Vector2\u0026gt;(new Vector2()); } } std::cout \u0026lt;\u0026lt; \u0026#34;Make unique\\n\u0026#34;; { std::array\u0026lt;std::unique_ptr\u0026lt;Vector2\u0026gt;, 1000\u0026gt; uniquePtrs; Timer timer; for (int i = 0; i \u0026lt; uniquePtrs.size(); i++) { uniquePtrs[i] = std::unique_ptr\u0026lt;Vector2\u0026gt;(); } } return 0; } ","date":"2023-07-13T15:25:06+10:30","image":"https://jiajun2001.github.io/p/c-study-note/cpp_hu_93cf5596b65127c7.png","permalink":"https://jiajun2001.github.io/p/c-study-note/","title":"C++ Study Note"},{"content":"Upon completing my penultimate year of studying for a Bachelor of Computer Science at the University of Adelaide, I was fortunate to be awarded the Adelaide Summer Research Scholarship for the period of 2022-2023. Delving into a 6-week research project, I collaborated with my research partner, Jian Zhe Chan, while being guided by the experienced supervision of Dr. Cruz Izu and Dr. Amali Weerasinghe.\nOur research endeavor aimed to facilitate the utilization of code quality feedback tools for novice programmers. The culmination of our efforts resulted in the development of \u0026ldquo;CPPAnalyzer\u0026rdquo;, a C++ code quality checker tailored explicitly for novice programmers. This tool serves as a valuable resource to enhance coding style and foster good programming habits among beginners.\nIn this post, I am delighted to present a comprehensive summary of our research and serve as a documentation guide for using the \u0026ldquo;CPPAnalyzer\u0026rdquo; code quality checker. Our hope is that this tool will be a stepping stone for aspiring programmers to sharpen their skills and embark on a journey of coding excellence.\nResearch Topic Facilitate the use of code quality feedback tools for novice programmers Supervisors Dr Cruz Izu Lecturer School of Computer and Mathematical Sciences, Faculty of Sciences, Engineering and Technology Research Interests: Computer System Architecture, Higher Education, Networking and Communications, Teacher Education and Professional Development of Educators Dr Amali Weerasinghe School of Computer and Mathematical Sciences, Faculty of Sciences, Engineering and Technology Research Interests: Artificial Intelligence Student Researchers Jiajun Yu (Jason) Bachelor of Computer Science Interests: Computer Systems, Computer Architecture, Algorithms, Data Structures Email: jiajun.yu@student.adelaide.edu.au Blog: https://jiajun2001.github.io/ Jian Zhe Chan (JZ) Bachelor of Computer Science Interests: System Programming, Competitive Programming, Computer Networks and Applications, Parallel and Distributed Systems Email: jianzhe.chan@student.adelaide.edu.au Blog: https://chan-jz.github.io/ Summer Research Outcome Familiarized with the use and features of two code quality checking tools. Used BASH scripts and Python programs to analyze two code quality checking tools (Pylint and Hyperstyle) outputs generated from codes submitted by students (Jupyter Notebook, Python, Java files), and used excel to visualize the data. Developed a tool to filter Pylint error codes in groups for teaching and researching purposes. Developed a tool (CPPAnalyzer) to pick up code smells that are missed by existing code quality checking tools About CPPAnalyzer CPPAnalyzer is a code quality checking tool that is developed by Jiajun Yu and Jian Zhe Chan under the supervison of Dr Cruz Izu and Dr Amali Weerasinghe during the summer of 2022-2023. It is aimed to analyze the code quality of C++ programs submitted by novice programmers and detect or pick up details missed by existing tools to help students have good coding styles and habits. Originally, CPPAnalyzer was planned and desigend to detect 10 rules (FYI: Summer-Research/CPPAnalyzer/Refactor_patterns.pdf). However, due to the time constraint and other unexpected situations, JZ and me only finished implementing 6 rules, they were: Refactor Rule 1: Simplify Boolean Expression Refactor Rule 2: Simplify Boolean Return Refactor Rule 3: Collapsible Nested if Statement Refactor Rule 8: Empty if Statement Refactor Rule 9: Dead Code And the rest unimplemented refactor rules are: Refactor Rule 4: Consolidate Conditional Expressions Refactor Rule 5: Redundant Conditional Check Refactor Rule 6: No Use of Else Refactor Rule 7: Duplicate if-else Body In general, CPPAnalyzer is written in Python and it detects broken rules by checking all related nodes (if nodes, while nodes, for nodes, etc). We collect those nodes in JSON format which come from the abstract syntax trees of authentic student submissions generated by clang compiler. Let\u0026rsquo;s run CPPAnalyzer Clone this github repository using the following command 1 git clone --recursive https://github.com/jiajun2001/Summer-Research.git Go to CPPAnalyzer using the following command 1 cd CPPAnalyzer 3: We have provided you with some test cases in the CPPAnalyzer folder Examples: test_Rule3.cpp, test_Rule8.cpp, test_Rule9.cpp, test_Rule123.cpp 4: If you want to play around with them, firstly open ASTProcessor.py file. This is the entry point of our CPPAnalyzer. 5: Modify line 9 to specify your input file (by default, the input C++ file for testing is test_Rule123.cpp) fileName = \u0026quot;test_Rule123.cpp\u0026quot; 6: Now we can run the program by simply entering the following command: 1 python ASTProcessor.py 7: If this command does not work, try this command: 1 python3 ASTProcessor.py If this still does not work, you can check your environment to see if you have properly installed Python or not. 8: Have great fun! If you follow the steps above correctly, you should technically see something like this in your terminal: 1 2 3 4 5 6 7 8 9 10 (base) apple@student-10-201-17-110 CPPAnalyzer % python3 ASTProcessor.py test_Rule123.cpp:13:9: RRE001: No need to check if Boolean variable or expression is equal to true or false (Simplify Boolean Expression) test_Rule123.cpp:19:12: RRE001: No need to check if Boolean variable or expression is equal to true or false (Simplify Boolean Expression) test_Rule123.cpp:19:27: RRE001: No need to check if Boolean variable or expression is equal to true or false (Simplify Boolean Expression) test_Rule123.cpp:20:19: RRE001: No need to check if Boolean variable or expression is equal to true or false (Simplify Boolean Expression) test_Rule123.cpp:20:35: RRE001: No need to check if Boolean variable or expression is equal to true or false (Simplify Boolean Expression) test_Rule123.cpp:20:51: RRE001: No need to check if Boolean variable or expression is equal to true or false (Simplify Boolean Expression) test_Rule123.cpp:13:5: RRE003: Avoiding deep nested ifs by using and (\u0026amp;\u0026amp;) operator test_Rule123.cpp:21:13: RRE009A: Detected code that is never executed below the current return/continue/return statement (base) apple@student-10-201-17-110 CPPAnalyzer % How does this work? This is the structure of CPPAnalyzer: Workflow of CPPAnalyzer\nASTProcessor.py is the entry point of CPPAnalyzer. This file allows you to specify your input file (C++ program that you are going to analyze), and contains other collectors and rule detectors. If you are going to extend CPPAnalyzer, you should include and invoke your rule detectors in this file. In this file, we will also invoke a module called HeaderPreprocessor before calling collectors and rule detectors. When we are generating the AST (abstract syntax tree), we do not want the tree to include any libraries. Otherwise, the tree structure will be giant and messy and this does not help us analyze the code structure. Therefore, we used this module to disable any library that the author of the program included before. The idea here is by putting a line of mark code int XX_MARKER_XX; to indicate where should we start collecting useful subroutine(function) nodes. Under Tools folder, we have a folder for collectors and another folder for detectors. Technically, at first, we would invoke detectors in consecutive order (Rule1Detector -\u0026gt; Rule2Detector -\u0026gt; Rule3Detector\u0026hellip;) For each rule detector, it will invoke collectors to collect JSON nodes needed from the AST (abstract syntax tree) and store those JSON nodes in temp folder. For your reference, if you want to have a look at the AST generated, you could use the following command to dump one in temp folder. 1 clang -Xclang -ast-dump=json -fsyntax-only -fno-color-diagnostics test_Rule123.cpp \u0026gt; temp/output.json After obtaining all nodes we need for detecting one rule, we could start coding the main logic for checking if the node breaks the rule or not. The general ideas here is to run a for loop to check each relative node that we collected before. Since code structure can sometimes be nested, so we used a few recursive functions (DFS and BFS) to analyze a single node. Genuine Advice Try to let the program run first and play around with it by changing the input code to see what will the terminal generate. When you are extending this project, try to use some very simple test cases and see the structures of the AST (abstract syntax tree) generated and nodes colleted by collectors. It is important to be familiar with the JSON files generated. You could use some online JSON parsers to visualize the structure. Consolidate your knowledge about tree data structure and tree traversal algorithms. Knowing these knowledge is of vital importance for further developing the project. We might be wrong in some stages of programming this CPPAnalyzer, so feel free to apply your own ideas and change some existing code. Good luck! ","date":"2023-07-08T17:11:55+10:30","image":"https://jiajun2001.github.io/p/adelaide-summer-research-project/background_hu_5054bb1b96c696e8.png","permalink":"https://jiajun2001.github.io/p/adelaide-summer-research-project/","title":"Adelaide Summer Research Project"},{"content":"\nMeasuring Performance Propagation Delay The time it takes for a signal to travel from the sender to the receiver in a communication system. Formula: Distance / Transmission speed Transmission Delay The time it takes to put bits (1/0) into a medium controlled by bandwidth of a link (data carrying capacity of a link) Formula: Data size / Link bandwidth Queuing Delay Queuing delay refers to the time a packet spends waiting in a queue before it can be transmitted or processed further. Processing Delay Processing delay refers to the time taken to process a packet at a network node before it can be forwarded or transmitted. Application Architecture Definition: Designed by the application developer and dictates how the application is structured over various end systems Client-server architecture (C/S) There is an always-on host, called the server, which serves requests from many other hosts, called clients. With the client-server architecture, clients do not directly communicate with each other. Peer-to-peer architecture (P2P) There is minimal (or no) reliance on dedicated servers in data centers. Instead the application exploits direct communication between pairs of intermittently connected hosts, called peers. Advantages: Scalability: P2P systems can be more scalable as the workload is distributed across multiple peers, and adding more peers can increase the system\u0026rsquo;s capability. Fault Tolerance: There is no central point of failure. Cost: P2P architectures are also cost effective since they normally don\u0026rsquo;t require significant server infrastructure and server bandwidth. Disadvantages: Faces challenges of security, performance, and reliability due to their high decentralized structure. Performance (time) Assume file size: F, server upload rate: us, N peers, lowest download rate of the peer: dmin C/S: max{ NF/us, F/dmin } P2P: max{ F/us, F/dmin, NF/(us + u1 + \u0026hellip; + uN) } C/S faster than P2P conditions Participating peers: In a P2P network, the availability of the file depends on the peers hosting it, and if these peers go offline or become unreliable, it can impact the file distribution speed Network infrastructure: If the network infrastructure is well-designed and optimized, a C/S architecture can take advantage of high speed connections and low latency Socket Programming Socket: A software interface that allows a process to send messages into, and receive messages from the network. A socket is the interface between the application layer and the transport layer within a host. Socket programming for C/S and P2P C/S: CS systems follow a centralized approach where a server listens on a specific socket for clients connections In CS architecture, the server creates and binds a socket to a specific IP address and port number. It listens for incoming client connections on this socket using functions like \u0026lsquo;socket()\u0026rsquo;, \u0026lsquo;bind()\u0026rsquo;, and \u0026rsquo;listen()\u0026rsquo;. Clients create a socket and connect it to the server\u0026rsquo;s IP address and port number using functions like \u0026lsquo;socket()\u0026rsquo; and \u0026lsquo;connect()\u0026rsquo;. Communication in CS architecture usually involves a request-response model, where clients send requests to the server, and the server responds to those requests. P2P: P2P systems are decentralized, and peers communicate directly with each other without relying on a central server. Each peer in a P2P network creates a socket to listen for incoming connections from other peers, as well as initiate connections to other peers. Socket programming for TCP and UDP TCP Server: socket(), bind(), listen() Client: socket(), connect(), send() UDP: (no connection between client and server) Server: socket(), bind() Client: socket(), sendto() Domain Name System (DNS) A DNS server provides name resolution (conversion from a domain name to an IP address). A name server is a process listening on UDP/TCP port 53 for requests. When a domain name is detected, it will be resolved and a reply will be sent. DNS hierarchical database Root name server: able to resolve all queries or another intermediate name server Top-level domain servers (TLD): responsible for com, org, net, edu, etc, and all top-level country domains uk, fr, ca, jp. Authoritative DNS servers: organization\u0026rsquo;s DNS servers, providing authoritative hostname to IP mappings for organization\u0026rsquo;s servers Local name server: handles local DNS requests. Caches resolved addresses. Queries Iterative: contacted server replies with name of server to contact Recursive: puts burden of name resolution on contacted name server DNS records (stored in distributed database) Resource records (RR) format: (name, value, type, ttl) Type = A: name = hostname, value = IP address Type = NS: name = domain, value = hostname of authoritative name server Type = CNAME: name = alias name, value: canonical name Type = MX: value = name of mail server associated with the name HTTP Behaviors of HTTP Requests methods: GET, POST, HEAD, PUT(update), DELETE(remove) Responses: 2xx(success), 3xx(redirection), 4xx(client error), 5xx(server error) Difference between HTTP 1.0 and HTTP 1.1 Persistent connections: HTTP 1.0 requires 2 RTTs per object. HTTP 1.1 allows multiple requests to be sent and received over the same connection, reducing the overhead of establishing new connections for each request. Pipelining: HTTP 1.1 enables sending multiple requests without waiting for each response. In HTTP 1.0, the client had to wait for a response before sending the next request. Host header: HTTP 1.1 introduced the host header which allows hosting multiple websites on a single IP address, and enables server to identify the appropriate website to handle the request. Caching/proxies: Goal: satisfy client request without involving origin server Advantages Reduce response time for client request Reduce traffic on an institution\u0026rsquo;s access link Enables \u0026lsquo;poor\u0026rsquo; content providers to effectively deliver content Transport Layer The transport layer is responsible for providing logical communication between processes. It uses the services of the network layer to transfer data between processes. Offered services: TCP: in-order delivery / connection oriented / reliable delivery UDP: out of order delivery / no connection establishment / best effort TCP and UDP both provide multiplexing and de-multiplexing of data from several processes Multiplexing at sender: handle data from multiple sockets, add transport header De-multiplexing at receiver: use header info to deliver received segments to correct socket Port numbers and IP address in TCP and UDP TCP: Datagram must specify: source IP address / source port number / destination IP address / destination port number UDP: Datagram must specify: destination IP address / destination port number UDP de-multiplexing: IP datagrams with same destination port, but different source IP address and/or source port numbers will be directed to same socket at destination Reliable data transfer rdt1.0: underlying channel perfectly reliable rdt2.0: underlying channel may flip bits in packet checksum to detect bit errors sender retransmits packet on receipt of NAK rdt2.1: handles garbled ACK/NAKs sender: sequence number added to packet / must check if received ACK/NAK corrupted receiver: must check if received packet is duplicated or not rdt2.2: a NAK-free protocal receiver sends ACK for last packet received OK (receiver must include sequence number of packet being ACKed) rdt3.0 (Alternating Bit Protocol): channels with errors and loss sender waits \u0026lsquo;reasonable\u0026rsquo; amount of time for ACK retransmits if no ACK received in this time Go-Back-N (GBN) One timer, on timeout, retransmit all packets in window from last ACK + 1 Receiver discards out of order packets and sends back cumulative ACK Selective Repeat (SR) Sender keeps track of the ACK received from the receiver and retransmits only the lost or damaged packets, instead of retransmitting the entire window of packets. Receiver acknowledges the receipt of each packet, and uses a buffer to store out-of-order packets. If the window size is N, we need 2N sequence numbers TCP TCP is designed to provide the appearance of a reliable channel over the unreliable network layer (IP). TCP is a modified hybrid of go-Back-N and selective repeat. TCP segment Source and destination ports: The combination of the source port and destination port fields allows TCP to establish and maintain multiple simultaneous connections between different applications or services running on the same or different hosts. SYN: indicating that this segment is a synchronous request to initialize a TCP connection. It is used during the TCP handshake process to establish a connection ACK (cumulative ACKS): indicating that this segment is an acknowledgment of a previously received segment. It confirms that the recipient has received the segment identified by the ACKNUM field SEQNUM: specifies the sequence number assigned by the sender to this segment. It is used to number the data bytes in a TCP stream. By using sequence numbers, TCP ensures that data is received in the correct order and any missing or duplicate segments can be detected. Window size: The window size field is a 16-bit value that indicates the amount of data, in bytes, that a receiver is willing to accept before requiring the sender to pause and wait for acknowledgement. ACKNUM: indicates the sequence number that the sender of this segment expects to receive next Checksum: used for error detection. The TCP checksum is a 16-bit field that helps ensure the integrity of the TCP segment during transmission. It is calculated by the sender and verified by the recipient to detect errors that may have occurred during transmission. Timeout formula: Timeout = EstimatedRTT + 4 * Deviation EstimatedRTT = (1 - x) * EstimatedRTT + x * SampleRTT Deviation = (1 - x) * Deviation + x * |SampleRTT - EstimatedRTT| TCP connection establishment and termination Connection TCP connections are established by a 3-way handshake: request connection, grant connection, acknowledge Why not 2 way handshake: 1: Server does not know the client is able to receive its message 2: If the first ACK is lost, client will send another request which will result in the server allocating more resources to make unnecessary establishment Why picking random values for starting sequence numbers It is possible that a server establishes two connections, retransmitted data will be delivered to the second connection Security issue, if an attacker knows all sequence numbers starting with the same value, he is able to inject some data Termination Client -\u0026gt; Server: FIN Server -\u0026gt; Client: ACK / FIN Client -\u0026gt; Server: ACK Time wait increases the chance of server receiving the final ACK TCP flow control Receiver informs sender of the receive window size in the header of TCP segments At the sender, LastByteSent - LastByteAcked \u0026lt;= Receive Window If the receive buffer is full, the sender will just send one byte TCP congestion control Slow start: the value of the congestion window begins at 1 MSS and increase by 1 MSS every time a transmitted segment is first acknowledged (grows exponentially). If there is a loss event indicated by a timeout, TCP sender sets the value of congestion window to be 1 and \u0026lsquo;ssthresh\u0026rsquo; to be \u0026lsquo;cwnd / 2\u0026rsquo;. Slow start will end when the value of \u0026lsquo;cwnd\u0026rsquo; equals \u0026lsquo;ssthresh\u0026rsquo;, or three duplicated ACKs detected. Congestion avoidance mode: TCP increases the value of \u0026lsquo;cwnd\u0026rsquo; by just a single MSS every RTT (grows linearly). If there is a loss event indicated by a timeout, TCP sender sets the value of congestion window to be 1 and \u0026lsquo;ssthresh\u0026rsquo; to be \u0026lsquo;cwnd / 2\u0026rsquo;. Congestion avoidance will end when a loss event occurs, or there are three duplicated ACKs detected. Fast recovery (TCP Reno incorporated): TCP sender sets \u0026lsquo;ssthresh\u0026rsquo; to be \u0026lsquo;cwnd / 2\u0026rsquo;, and \u0026lsquo;cwnd\u0026rsquo; to be ssthresh + 3 * MSS. Fast retransmit: For TCP Reno - 1990, if there are three duplicated ACKs detected, retransmit immediately. If there is timeout, TCP Reno will behave the same as TCP Tahoe. They behave differently when there are three duplicated ACKs detected. TCP Reno will go to fast recovery mode, while TCP Tahoe will go to slow start mode. Network Layer Datagram protocol A connectionless protocol where each packet, also known as a datagram, is treated independently and can take different routes to reach its destination. In the case of IP, each IP packet (datagram) contains the source and destination IP addresses, as well as the data payload. Limitation: Reliable delivery / ordering / congestion control / Fragmentation and reassembly Functions Path determination: route taken by packets from source to dest Forwarding: move packets from routers input to appropriate router output Call setup: some network architectures require router call setup along path before data flows Network layer Control plane network-wide logic determines how datagram is routed among routers along end-end path from source host to destination host Two control-plane approaches Traditional routing algorithms: implemented in routers Classification Global or decentralized Global: all routers have complete topology, link cost info (link state algorithms) Decentralized: router knows physically-connected neighbors, link cost to neighbors / Iterative process of computation, exchange of info with neighbors Static or dynamic Static: routers change slowly over time Dynamic: routers change more quickly (periodic update / in response to link cost changes) Dijkstra\u0026rsquo;s algorithm Complexity: O(n2), more efficient: O(nlogn). Requires O(nE) messages Might have oscillations Distance Vector algorithm Attributes iterative: continues until no nodes exchange info (self-terminating) asynchronous: nodes need not exchange info / iterate in lock step distributed: each node communicates only with directly-attached neighbors Formula DX(Y, Z) = distance from X to Y, via Z as next hop DX(Y, Z) = c(X, Z) + minw{DZ(Y, w)} Process Each node waits for message from neighbors Recomputes distance table If least cost path to any dest has changed, notify neighbors Convergence time varies May be routing loops / count-to-infinity problem Comparison Link state algorithm Faster convergence / Efficient routing Distance vector Simple to implement / Lower overhead Software-defined networking (SDN): implemented in remote servers Each router contains a flow table that is computed and distributed by a logically centralized routing controller programmable control applications Data plane local, per-router function determines how datagram arriving on router input port is forwarded to router output port forwarding table IP Addressing Classfull addressing Disadvantages: inefficient use of address space, address space exhaustion Class A: | 0 | network | host | host | host | Class B: | 10 | network | network | host | host | Class C: | 110 | network | network | network | host | Class D: | 1110 | multicast address | Classless InterDomain Routing (CIDR) network portion of address of arbitrary length address format: a.b.c.d/x, where x is the number of bits in network portion of address Subnetting The network administrator takes a network with a given IP address range and subnet mask and further divides it into smaller subnets by borrowing bits from the host portion of the IP address. Reserved address Network identifiers have a host part of all 0s Broadcast addresses have a host part of all 1s Dynamic Host Configuration Protocol (DHCP) Dynamically get an IP address (plug and play) Process Host broadcasts \u0026ldquo;DHCP discover\u0026rdquo; message DHCP server responds with \u0026ldquo;DHCP offer\u0026rdquo; message Host requests IP address: \u0026ldquo;DHCP request\u0026rdquo; message DHCP server sends address: \u0026ldquo;DHCP ACK\u0026rdquo; message Intra-AS Routing (Interior Gateway Protocol - IGP): Intra-AS routing refers to the routing protocols and mechanisms used within an individual Autonomous System (AS). All routers in AS must run same intra-domain protocol. An AS is a collection of networks under a single administrative control, such as an Internet Service Provider (ISP) or a large enterprise network. Intra-AS routing protocols, also known as Interior Gateway Protocols (IGPs), are used to exchange routing information and determine the best paths for routing packets within the boundaries of the AS. Inter-AS Routing (Exterior Gateway Protocol - EGP): Inter-AS routing refers to the routing protocols and mechanisms used for exchanging routing information between different Autonomous Systems (ASes) in the Internet. Inter-AS routing protocols, also known as Exterior Gateway Protocols (EGPs), are responsible for facilitating the exchange of routing information and enabling communication between ASes. Fragmentation Large IP datagram divided (fragmented) within net One datagram becomes several datagrams Reassembled only at final destination IP header bits used to identify order related fragments 16-bit identifier: Examine the identification numbers of the datagrams to determine which of the datagrams are actually fragments of the same larger datagram Flags: In order for the destination host be absolutely sure it has received the last fragment of the original datagram, the last fragment has a flag bit set to 0, whereas all the other fragments have this flag bit set to 1 Fragment offset: Specify where the fragment fits within the original IP datagram IPv6 Why do we transition to IPv6 Address space exhaustion / Enhance security / Improved network performance Features 128 bit address are written in hex: x : x : x : x : x : x : x : x, each x is 4 hex digits, sequence of zero fields given by \u0026ldquo;::\u0026rdquo; IPv6 8 fields in base header vs 13 fields in IPv4 can provide faster processing, simpler management, and more flexibility Fragmentation is no longer performed at intermediate routers. The source host should choose datagram size so fragmentation is not necessary. (e.g. send datagram with different sizes to target until they don\u0026rsquo;t arrive) No checksum design reduces the processing overhead on intermediate routers and simplifies packet forwarding. The link-layer protocols in modern networks, such as Ethernet, already have their own error detection mechanisms, such as CRC (Cyclic Redundancy Check), which ensure data integrity during transmission. Additional support Multicast and anycast routing Security Mobile hosts and auto-configuration Real time applications Transition from IPv4 to IPv6 Dual stack: some routers with dual stack can translate between formats (tend to lose some functionalities) Tunneling: IPv6 carried as payload in IPv4 datagram among IPv4 routers Internet Control Message Protocol (ICMP) ICMP is primarily used for diagnostic and error reporting purposes in IP networks. It provides a means for network devices, such as routers and hosts, to communicate with each other regarding network-related issues. Main functions Error Reporting: ICMP is used to report errors and anomalies in IP packet delivery. For example, if a router encounters a problem while forwarding an IP packet, it can generate an ICMP error message and send it back to the source IP address to inform the sender about the issue. Network Reachability: ICMP is used to check the reachability of hosts or networks. The most common example is the ICMP Echo Request and Echo Reply messages, also known as \u0026ldquo;ping.\u0026rdquo; By sending an Echo Request message, a device can determine if another device on the network is reachable and functioning properly. Path MTU Discovery: ICMP is involved in the process of Path Maximum Transmission Unit (PMTU) Discovery. It helps determine the maximum size of IP packets that can be transmitted without fragmentation along a path between two hosts. ICMP messages, such as \u0026ldquo;Packet Too Big,\u0026rdquo; are used to inform the sender that the packet is too large and needs to be fragmented or reduced in size. Trace the path taken by a packet over the network: By sending packets with increasing TTL values, traceroute effectively maps the network path from the source to the destination. The ICMP \u0026ldquo;Time Exceeded\u0026rdquo; messages indicate the presence of routers along the path, while the \u0026ldquo;Echo Reply\u0026rdquo; message confirms the packet\u0026rsquo;s successful arrival at the destination. Link Layer Data-link layer has responsibility of transferring datagram from one node to physically adjacent node over a link Offered services: Framing Link access Reliable delivery between two physically connected devices Flow control Error detection / correction Error detection and correction Parity Single bit parity (detect single bit errors) odd parity: number of 1, if odd, parity bit = 0 even parity: number of 1, if odd, parity bit = 1 Two dimensional bit parity (detect and correct single bit errors) Checksums Errors do not occur as a one-off single bit error, we normally have an error burst Internet checksum is 1\u0026rsquo;s compliment sum of the segment contents When information is received, sum the contents and add with the checksums. If it is \u0026ldquo;FFFFFFFF\u0026rdquo;, then the information received is correct Cyclic Redundancy Check polynomials (CRCs) Choose r + 1 bit pattern (Generator) G Goal: choose r CRC bits, R such that \u0026lt;D, R\u0026gt; exactly divisible by G receiver knows G and divides \u0026lt;D, R\u0026gt; by G can detect all burst errors less that r + 1 bits If G(x) with degree of r (length r + 1), then it is guaranteed to detect burst length errors of r or less Advantages: CRCs are popular because they are simple to implement in binary hardware, easy to analyze mathematically, and particularly good at detecting common errors caused by noise in transmission channels. Multiple access protocols Channel partitioning Frequency Division Multiple Access (FDMA) Channel spectrum divided into frequency bands Each station assigned fixed frequency band Unused transmission time in frequency bands go idle Time Division Multiple Access (TDMA) Access to channel in \u0026lsquo;rounds\u0026rsquo; Each station gets fixed length slot (length = packet transmission time) in each round Unused slots go idle Code Division Multiple Access (CDMA) Unique code assigned to each user to encode data Allows multiple users to \u0026lsquo;coexist\u0026rsquo; and transmit simultaneously with minimal interference (if codes are orthogonal) Random access Slotted ALOHA Time is divided into equal size slots Node with new arriving packets: transmit at beginning of next slot If there is a collision: re-transmit the packet in future slots with probability p, until successful At best: channel used for useful transmissions 37% of time (Np = 1) Pure (unslotted) ALOHA Transmit immediately without awaiting for beginning of slot Collision probability increases: frame sent at t0 collides with other packets sent in [t0 - 1, t0 + 1] At best: 18% of time (Np = 0.5) Carrier Sense Multiple Access (CSMA) Listen before transmit Types of CSMA Persistent CSMA: retry immediately with probability p when channel becomes idle Non-persistent CSMA: retry after random interval CSMA collisions Collisions can occur due to propagation delay Collisions means entire packet transmission time is wasted - up to 2 packet times Carrier Sense Multiple Access with Collision Detection (CSMA/CD) Colliding transmissions aborted, reducing channel wastage Taking turns Polling Master node invites slave nodes to transmit in turn Token passing Control token passed from one node to next sequentially Distributed polling Begins with N short reservation slots Stations with message to send posts reservation Reservation seen by all stations After reservation slots, message transmissions ordered by known priority LAN / MAC / physical address Used locally to get frame from one interface to another physically-connected interface Format: 48 bit MAC address burned in the adapter ROM (hexadecimal notation) Address Resolution Protocol (ARP) Given a destination IP address, work out the MAC address of the destination ARP Table: IP/MAC address mappings for some LAN nodes Table entry format: \u0026lt;IP Address; MAC Address; TTL\u0026gt; Process 1: A broadcasts ARP query packet, containing B\u0026rsquo;s IP address 2: B receives ARP packet, replies to A with its MAC address 3: A caches IP-to-MAC address pair in its ARP table until timeout Ethernet Dominant wired LAN technology Sending adapter encapsulates IP datagram in Ethernet frame Physical topology Bus: all nodes in same collision domain (can collide with each other | CSMA/CD) Star: active switch in center / each \u0026ldquo;spoke\u0026rdquo; runs a separate Ethernet protocol (nodes do not collide with each other) Switches Functions Filtering, storing, forwarding Ethernet frames Examine incoming frame\u0026rsquo;s MAC address, selectively forward frame to one-or-more outgoing links when frame is to be forwarded on segment. Plug and play, self-learning Switch table entry: [node LAN address, switch interface, time stamp, TTL] If a frame\u0026rsquo;s destination is unknown, flood (flood learning) Cycles result organize switches in a spanning tree by disabling subset of interfaces The Spanning Tree Protocol (STP) is a network protocol used in Layer 2 Ethernet networks to prevent loops and ensure the creation of a loop-free topology. Its main role is to establish a tree-like structure within a network by selectively blocking redundant paths. MPLS Initial goal: high-speed IP forwarding using fixed length labels Fast lookup using fixed length identifier (rather than longest prefix matching) MPLS routing: path to destination can be based on source and destination addresses Fast reroute: pre-compute backup routes in case of link failure MPLS operations: push / swap / pop MPLS equipment: Label Edge Routers / Label Switch Routers MPLS features Traffic Engineering: MPLS allows network administrators to control the path and flow of traffic through the network. Flexibility: MPLS forwarding decisions can differ from those of IP Use destination and source addresses to route flows to same destination differently. Re-route flows quickly if link fails: pre-computed backup paths MPLS can support multiple levels of connection tunnelling through label stacking (VPN support) Traceroute A network diagnostic tool used to trace the path that an IP packet takes from a source device to a destination device over an IP network. It provides valuable information about the routers or network devices traversed by the packet, helping identify network connectivity issues, bottlenecks, and routing problems Security Confidentiality: only sender, intended receiver should understand message contents Sender encrypts message / Receiver decrypts message Authentication: sender, receiver want to confirm identity of each other Message integrity: sender, receiver want to ensure message not altered without detection Access and availability: services must be accessible and available to users Attacks Eavesdrop: intercept messages Actively insert messages into connections Impersonation: Fake (spoof) source address in packet (or any field in packet) Hijacking: take over ongoing connection by removing sender or receiver, inserting himself in place Deny of service: prevent service from being used by others Nonce: number used only once-in-a-lifetime Challenge-response protocols are authentication protocols that involve a challenge presented by the verifier to the prover, who must provide a response that proves their identity or possession of a secret key. Hash function produces fixed-size message digest (fingerprint) given message digest x, computationally infeasible to find m such that x = H(m) process data quickly and hashed value is efficient for verification Public key encryption algorithm Requirements 1: need K+(B) and K-(B) such that K-(K+(m)) = m 2: given public key K+(B), it should be impossible to compute private key K-(B). Important property K-(K+(m)) = m = K+(K-(m)) Certification authority (CA): binds public key to particular entry, E. E registers its public key with CA by providing \u0026ldquo;proof of identity\u0026rdquo; to CA CA creates certificate binding E to its public key Key distribution problem: refers to the challenge of securely distributing encryption keys between parties involved in secure communication or cryptographic systems. The key distribution problem arises from the need to establish and exchange encryption keys in a secure and reliable manner. Secure e-mail Alice wants to provide confidentiality, sender authentication, message integrity. Confidentiality: encryption Sender authentication: digital signature Message integrity: Hashing Approach Ks{K-A(H(m)), m}, K+B(Ks) ","date":"2023-06-18T12:00:00+10:30","image":"https://jiajun2001.github.io/p/computer-network-and-application/cna_hu_abe7381da866f790.png","permalink":"https://jiajun2001.github.io/p/computer-network-and-application/","title":"Computer Network and Application"},{"content":"Complexity The worst-case complexity of the algorithm is the function defined by the maximum number of steps taken in any instance of size n.\nThe best-case complexity of the algorithm is the function defined by the minimum number of steps taken in any instance of size n.\nThe average-case complexity of the algorithm is the function defined by the average number of steps over all instances of size n.\nBig O Notation The big O notation simplifies analysis by ignoring levels of detail that do not impact the comparison of algorithms.\nTwo Types of Data Structures Contiguously-allocated structures are composed of single slabs of memory, and include arrays, matrices, heaps and hash tables.\nAdvantages: Constant-time access | Space efficiency | Memory locality Linked data structures are composed of distinct chunks of memory bound together by pointers, and include lists, trees, and graph adjacency lists.\nAdvantage: More freedom | Simpler insertions and deletions | Moving pointers is easier and faster Arrays Operation Unsorted Array Sorted Array Search(L, k) O(n) O(logn) Insert(L, x) O(1) O(n) Delete(L, x) O(1) O(n) Successor(L, x) O(n) O(1) Predecessor(L, x) O(n) O(1) Minimum(L) O(n) O(1) Maximum(L) O(n) O(1) When deleting an element x from an unsorted array with size n, we can write over array\\[x\\] with array[n], and decrement n. Linked List Operation Singly Unsorted List Doubly Unsorted List Singly Sorted List Doubly Sorted List Search(L, k) O(n) O(n) O(n) O(n) Insert(L, x) O(1) O(1) O(n) O(n) Delete(L, x) O(n) O(1) O(n) O(1) Successor(L, x) O(n) O(n) O(1) O(1) Predecessor(L, x) O(n) O(n) O(n) O(1) Minimum(L) O(n) O(n) O(1) O(1) Maximum(L) O(n) O(n) O(1) O(1) For deletion, assume we are given a pointer x to the item to be deleted. However, we actually need a pointer to the element pointing to x in the list. Therefore, the deletion time complexity for doubly linked list is always O(1).\nTo find the maximum element in a singly sorted list, we can maintain a separate pointer to the list tail. This will not change the cost for deletion.\nImplementation of singly linked list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class Node { int val; Node next; Node(int val) { this.val = val; } } class MyLinkedList { int size; Node dummyHead; public MyLinkedList() { size = 0; dummyHead = new Node(0); } public int get(int index) { if (index \u0026lt; 0 || index \u0026gt;= size) return -1; Node currentNode = dummyHead; for (int i = 0; i \u0026lt;= index; i++) { currentNode = currentNode.next; } return currentNode.val; } public void addAtHead(int val) { Node currentNode = dummyHead; Node newNode = new Node(val); newNode.next = currentNode.next; currentNode.next = newNode; size++; } public void addAtTail(int val) { Node currentNode = dummyHead; for (int i = 0; i \u0026lt; size; i++) { currentNode = currentNode.next; } Node newNode = new Node(val); currentNode.next = newNode; size++; } public void addAtIndex(int index, int val) { if (index \u0026lt; 0 || index \u0026gt; size) return; Node currentNode = dummyHead; for (int i = 0; i \u0026lt; index; i++) { currentNode = currentNode.next; } Node newNode = new Node(val); newNode.next = currentNode.next; currentNode.next = newNode; size++; } public void deleteAtIndex(int index) { if (index \u0026lt; 0) index = 0; if (index \u0026gt;= size) return; Node currentNode = dummyHead; for (int i = 0; i \u0026lt; index; i++) { currentNode = currentNode.next; } currentNode.next = currentNode.next.next; size--; } } Implementation of doubly linked list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class Node { int val; Node prev, next; Node(int val) { this.val = val; } } class MyLinkedList { int size; Node dummyHead; Node dummyTail; public MyLinkedList() { size = 0; dummyHead = new Node(0); dummyTail = new Node(0); dummyHead.next = dummyTail; dummyTail.prev = dummyHead; } public int get(int index) { if (index \u0026lt; 0 || index \u0026gt;= size) return -1; Node currentNode; if (index \u0026gt;= size / 2) { currentNode = dummyTail; for (int i = 0; i \u0026lt; size - index; i++) { currentNode = currentNode.prev; } } else { currentNode = dummyHead; for (int i = 0; i \u0026lt;= index; i++) { currentNode = currentNode.next; } } return currentNode.val; } public void addAtHead(int val) { Node currentNode = dummyHead; Node newNode = new Node(val); newNode.next = currentNode.next; newNode.next.prev = newNode; currentNode.next = newNode; newNode.prev = currentNode; size++; } public void addAtTail(int val) { Node currentNode = dummyTail; Node newNode = new Node(val); newNode.prev = currentNode.prev; newNode.prev.next = newNode; currentNode.prev = newNode; newNode.next = currentNode; size++; } public void addAtIndex(int index, int val) { if (index \u0026lt; 0 || index \u0026gt; size) return; Node currentNode; if (index \u0026gt;= size / 2) { currentNode = dummyTail; for (int i = 0; i \u0026lt; size - index + 1; i++) { currentNode = currentNode.prev; } } else { currentNode = dummyHead; for (int i = 0; i \u0026lt; index; i++) { currentNode = currentNode.next; } } Node newNode = new Node(val); newNode.next = currentNode.next; newNode.next.prev = newNode; currentNode.next = newNode; newNode.prev = currentNode; size++; } public void deleteAtIndex(int index) { if (index \u0026lt; 0 || index \u0026gt;= size) return; Node currentNode; if (index \u0026gt;= size / 2) { currentNode = dummyTail; for (int i = 0; i \u0026lt; size - index; i++) { currentNode = currentNode.prev; } } else { currentNode = dummyHead; for (int i = 0; i \u0026lt;= index; i++) { currentNode = currentNode.next; } } currentNode.next.prev = currentNode.prev; currentNode.prev.next = currentNode.next; size--; } } Implementation of Binary Search Tree 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class Node { int val; Node left, right; public Node(int val) { this.val = val; left = right = null; } } class BST { Node root; // Insert elements public Node insert(Node current, int val) { if (current == null) return new Node(val); if (val \u0026lt; current.val) { current.left = insert(current.left, val); } else if (val \u0026gt; current.val) { current.right = insert(current.right, val); } return current; } // Insert Helper Function public void insertHelper(int val) { root = insert(root, val); } // Search Function public boolean search(Node current, int val) { if (current == null) return false; if (current.val == val) return true; if (val \u0026lt; current.val) { return search(current.left, val); } else { return search(current.right, val); } } // Search Helper Function public void searchHelper(int val) { System.out.println(search(root, val)); } // FindMax function public Node findMax(Node current) { while (current.right != null) { current = current.right; } return current; } // Delete Function public Node delete(Node current, int val) { if (current == null) return null; if (val \u0026lt; current.val) { current.left = delete(current.left, val); } else if (val \u0026gt; current.val) { current.right = delete(current.right, val); } else { if (current.left == null \u0026amp;\u0026amp; current.right == null) { current = null; } else if (current.left == null) { current = current.right; } else if (current.right == null) { current = current.left; } else { Node temp = findMax(current.left); current.val = temp.val; current.left = delete(current.left, temp.val); } } return current; } // Delete Helper Function public void deleteHelper(int val) { root = delete(root, val); } // InOrder Traversal public void InOrder(Node current) { if (current == null) return; InOrder(current.left); System.out.print(current.val + \u0026#34; \u0026#34;); InOrder(current.right); } } Hashing Tables Closed addressing hashing (Open Hashing) handles collision by storing all elements with the same hashed key in one table entry.\nOpen addressing hashing (Closed Hashing) handles collision by storing subsequent elements with the same hashed key in different table entries.\nOperation Hash Table (expected) Hash Table (worst case) Search(L, k) O(n/m) O(n) Insert(L, x) O(1) O(1) Delete(L, x) O(1) O(1) Successor(L, x) O(n+m) O(n+m) Predecessor(L, x) O(n+m) O(n+m) Minimum(L) O(n+m) O(n+m) Maximum(L) O(n+m) O(n+m) Using chaining with doubly-linked lists to resolve collisions in an m-elements hash table with n items Heap Heap is a simple and elegant data structure for efficient supporting the priority queue operations insert and extract-min/max. They work by maintaining a partial order on the set of elements which is weaker than the sorted order.\nA heap-labeled tree is defined to be a binary tree such that the key labeling of each node dominates the key labeling of each of its children.\nIn a min-heap, a node dominates its children by containing a smaller key than they do.\nIn a max-heap, parent nodes dominate by begin bigger.\nHeaps are mostly used to implement Priority Queues.\nHeap Implementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 import java.util.Arrays; public class MinHeap { int size; int maxSize; int[] heap; MinHeap(int maxSize) { if (maxSize \u0026lt;= 0) return; this.maxSize = maxSize; this.size = 0; this.heap = new int[maxSize + 1]; Arrays.fill(heap, Integer.MAX_VALUE); this.heap[0] = Integer.MIN_VALUE; } MinHeap(int maxSize,int[] arr) { if (arr.length \u0026gt; maxSize) return; this.maxSize = maxSize; this.size = arr.length; this.heap = new int[maxSize + 1]; Arrays.fill(heap, Integer.MAX_VALUE); this.heap[0] = Integer.MIN_VALUE; for (int i = 0; i \u0026lt; arr.length; i++) { this.heap[i + 1] = arr[i]; } buildHeap(); } int parent(int pos) { return pos / 2; } int leftChild(int pos) { return pos * 2; } int rightChild(int pos) { return pos * 2 + 1; } boolean isLeaf(int pos) { return pos \u0026gt; size / 2 \u0026amp;\u0026amp; pos \u0026lt;= size; } void swap(int pos1, int pos2) { int temp = heap[pos1]; heap[pos1] = heap[pos2]; heap[pos2] = temp; } void minHeapify(int pos) { if (!isLeaf(pos)) { if (heap[pos] \u0026gt; heap[leftChild(pos)] || heap[pos] \u0026gt; heap[rightChild(pos)]) { int smallerElementIndex = heap[leftChild(pos)] \u0026lt; heap[rightChild(pos)] ? leftChild(pos) : rightChild(pos); swap(pos, smallerElementIndex); minHeapify(smallerElementIndex); } } } void insert(int element) { if (size \u0026gt;= maxSize) return; heap[++size] = element; int current = size; while (heap[parent(current)] \u0026gt; heap[current]) { swap(parent(current), current); current = parent(current); } } void buildHeap() { for (int i = size / 2; i \u0026gt;= 1; i--) { minHeapify(i); } } void removeMin() { if (size == 0) return; heap[1] = heap[size]; heap[size--] = Integer.MAX_VALUE; minHeapify(1); } // Function to print the contents of the heap void display() { if (size == 0) { System.out.println(\u0026#34;Empty Heap!\u0026#34;); return; } System.out.println(\u0026#34;PARENT\u0026#34; + \u0026#34;\\t\u0026#34; + \u0026#34;LEFT\u0026#34; + \u0026#34;\\t\u0026#34; + \u0026#34;RIGHT\u0026#34;); if (size == 1) { System.out.println(\u0026#34; \u0026#34; + heap[1] + \u0026#34;\\t\\t\u0026#34;); return; } for (int i = 1; i \u0026lt;= size / 2; i++) { System.out.print(\u0026#34; \u0026#34; + heap[i] + \u0026#34;\\t\\t\u0026#34; + heap[2 * i] + \u0026#34;\\t\\t\u0026#34; + heap[2 * i + 1]); System.out.println(); } } } When passing parameters into the function of \u0026lsquo;swap(int pos1, int pos2)\u0026rsquo;, we must be aware that the function handles indices instead of elements. Runtime of Heap Operations Operation Runtime Find the minimum element O(1) Delete minimum element O(logn) Insert an element O(logn) Build a heap O(n) Heap Sort Build the heap for n elements in time O(n)\nEach step picks and deletes the minimum element in time O(logn)\nIterate until the heap is empty.\nIn total n iterations implies a total runtime of O(nlogn)\nSelection Sort 1 2 3 4 5 6 7 8 9 10 11 private static void selection(int[] array) { for (int i = 0; i \u0026lt; array.length - 1; i++) { int min = i; for (int j = i + 1; j \u0026lt; array.length; j++) { if (array[j] \u0026lt; array[min]) min = j; } int temp = array[i]; array[i] = array[min]; array[min] = temp; } } Insertion Sort 1 2 3 4 5 6 7 8 9 10 11 private static void InsertionSort(int[] array) { for (int i = 1; i \u0026lt; array.length; i++) { int j = i; while (j \u0026gt; 0 \u0026amp;\u0026amp; array[j - 1] \u0026gt; array[j]) { int temp = array[j]; array[j] = array[j - 1]; array[j - 1] = temp; j--; } } } Merge Sort Merge sort involves partitioning the elements into two groups, sorting each of the smaller problems recursively, and then interleaving the two sorted lists to totally order the elements. It is a classic divide-and-conquer algorithm but its primary disadvantage is the need for an auxiliary array when sorting arrays.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import java.util.ArrayList; import java.util.Arrays; public class MergeSort { public ArrayList\u0026lt;Integer\u0026gt; mergeSort(ArrayList\u0026lt;Integer\u0026gt; myArr) { if (myArr.size() == 1) return myArr; int mid = myArr.size() / 2; ArrayList\u0026lt;Integer\u0026gt; left = new ArrayList\u0026lt;Integer\u0026gt;(); ArrayList\u0026lt;Integer\u0026gt; right = new ArrayList\u0026lt;Integer\u0026gt;(); for (int i = 0; i \u0026lt; mid; i++) { left.add(myArr.get(i)); } for (int i = mid; i \u0026lt; myArr.size(); i++) { right.add(myArr.get(i)); } ArrayList\u0026lt;Integer\u0026gt; lSplit = mergeSort(left); ArrayList\u0026lt;Integer\u0026gt; rSplit = mergeSort(right); return merge(lSplit, rSplit); } public ArrayList\u0026lt;Integer\u0026gt; merge(ArrayList\u0026lt;Integer\u0026gt; left, ArrayList\u0026lt;Integer\u0026gt; right) { int i = 0; int j = 0; ArrayList\u0026lt;Integer\u0026gt; outcome = new ArrayList\u0026lt;Integer\u0026gt;(); while (i \u0026lt; left.size() \u0026amp;\u0026amp; j \u0026lt; right.size()) { if (left.get(i) \u0026lt; right.get(j)) { outcome.add(left.get(i++)); } else { outcome.add(right.get(j++)); } } while (i \u0026lt; left.size()) { outcome.add(left.get(i++)); } while (j \u0026lt; right.size()) { outcome.add(right.get(j++)); } return outcome; } } Quick Sort 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import java.util.ArrayList; import java.util.Arrays; import java.util.Collections; public class QuickSort { int partition(ArrayList\u0026lt;Integer\u0026gt; arr, int start, int end) { int pivot = arr.get(end); int index = start; for (int i = start; i \u0026lt; end; i++) { if (arr.get(i) \u0026lt; pivot) { Collections.swap(arr, i, index); index++; } } Collections.swap(arr, index, end); return index; } void quickSort(ArrayList\u0026lt;Integer\u0026gt; arr, int start, int end) { if (start \u0026lt; end) { int partition = partition(arr, start, end); quickSort(arr, start, partition - 1); quickSort(arr, partition + 1, end); } } } Topological Sort 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 public class TopologicalSort { HashSet\u0026lt;Integer\u0026gt; visited; LinkedList\u0026lt;Integer\u0026gt;[] adjacencyList; LinkedList\u0026lt;Integer\u0026gt; myStack; int nodeNum; public TopologicalSort(int nodeNum) { visited = new HashSet\u0026lt;\u0026gt;(); adjacencyList = new LinkedList[nodeNum]; for (int i = 0; i \u0026lt; nodeNum; i++) { adjacencyList[i] = new LinkedList\u0026lt;\u0026gt;(); } myStack = new LinkedList\u0026lt;\u0026gt;(); this.nodeNum = nodeNum; } public void sort() { for (int i = 0; i \u0026lt; nodeNum; i++) { if (!visited.contains(i)) { sortUntil(i); } } } public void sortUntil(int node) { visited.add(node); for (int n : adjacencyList[node]) { if (!visited.contains(n)) { sortUntil(n); } } myStack.push(node); } public void addEdge(int parent, int child) { adjacencyList[parent].add(child); } } Backtracking Backtracking is a systematic way to iterate through all the possible configurations of a search space. These configuration may represent all possible arrangements of objects. Backtracking ensures correctness by enumerating all possibilities. It ensures efficiency by never visiting a state more than once. Dynamic Programming Dynamic programming gives us a way to design custom algorithms that systematically search all possibilities while storing results to avoid recomputimg. By storing the consequences of all possible decisions and using this information in a systematic way, the total amount of work is minimized. Essentially, dynamic programming is a trade off of space for time. Trie Trie is a tree data structure used for locating specific keys from a set. It is also known as prefix tree or digital tree.\nTrie can be used to build Auto-complete and Spell-checker.\nIt allows efficient storage of words with common prefixes. Each node in a trie represents a character in the string and also indicates the termination of the string.\nThere are three main functions: insert(word), search(word), startsWith(word).\nMain advantage of trie: startsWith() is very efficient.\nThe time complexity of insert, search and startWith are all O(k), where k is the length of the string.\nImplementation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class Node { HashMap\u0026lt;Character, Node\u0026gt; children; boolean endOfWord; Node() { this.children = new HashMap\u0026lt;Character, Node\u0026gt;(); this.endOfWord = false; } } class Trie { Node root; public Trie() { root = new Node(); } public void insert(String word) { Node cur = root; for (int i = 0; i \u0026lt; word.length(); i++) { char c = word.charAt(i); if (!cur.children.containsKey(c)) { cur.children.put(c, new Node()); } cur = cur.children.get(c); } cur.endOfWord = true; } public boolean search(String word) { Node cur = root; for (int i = 0; i \u0026lt; word.length(); i++) { char c = word.charAt(i); if (!cur.children.containsKey(c)) return false; cur = cur.children.get(c); } return cur.endOfWord; } public boolean startsWith(String prefix) { Node cur = root; for (int i = 0; i \u0026lt; prefix.length(); i++) { char c = prefix.charAt(i); if (!cur.children.containsKey(c)) return false; cur = cur.children.get(c); } return true; } } Amortized Analysis Amortized analysis is used for algorithms where an occasional operation is very slow, but most of the other operations are faster. In amortized analysis, we analyze a sequence of operations and guarantee a worst-case average time that is lower than the worst case of a particular expensive operation. (GeeksforGeeks)\nA good example is to analyze the dynamic array. For dynamic array, we need to double the size of the array whenever it becomes full. If the array becomes full, we need to allocate memory for larger array size, typically twice the old array, copy the contents of the old array to a new array, and free the old array. All of the steps above can take O(n). If we use simple analysis, we can draw the conclusion that the worst-case cost of n insertions is n * O(n) which is O(n^2). This analysis gives an upper bound, but not a tight upper bound for n insertions as all insertions don’t take Θ(n) time.\nHowever, by using amortized analysis, we list the cost of inserting each element and divide the sum of the insertion cost by the total number of insertions, we can get the amortized cost and find that the time complexity of dynamic array insertion is actually O(1).\nBit Manipulation Shifting For positive numbers:\nLeft Shift: Multiplying a number by 2. Right Shift: Divide a number by 2 (Round Down: 3 -\u0026gt; 1) For negative numbers:\nLogical Right Shift: Adding 0 in front (Get a meaningless number). Arithmatic Right Shift: Adding its original sign bit in front (Round Down: -5 -\u0026gt; -3). Masking Get c-th bit: ((1 \u0026laquo; c) \u0026amp; x) != 0 Set c-th bit to be 1: (1 \u0026laquo; c) | x Set c-th bit to be 0: (~(1 \u0026laquo; c)) \u0026amp; x Singleton Design Pattern Singleton design pattern is used when we need to ensure that only one object of a particular class need to be created. All further reference to this object are referred to the same underlying instance created.\nSingleton classes are used for logging, driver objects, caching and thread pool, database connections.\nIssues: Coupling issue / Concurrency issue\nFactory Design Pattern Factory design pattern deals with the problem of creating objects without having to specify the exact class of the object that will be created. This is done by creating objects by calling a factory method.\nWe use it when a method returns one of several possible classes that share a common parent class.\nAll potential classes are in the same subclass hierarchy.\nUnion Find A disjoint-set data structure is defined as a data structure that keeps track of set of elements partitioned into a number of disjoint subsets.\nA union-find algorithm is an algorithm that performs two useful operations on such a data structure:\nFind: Determine which subset a particular element is in. This can also be used for determining if two elements are in the same subset. Union: Join two subsets into a single subset. For naïve linking, a Union or Find operation can take O(n) time in the worst case, where n is the number of elements.\nProof: Find takes proportional time to the height of the tree. In the worst case, the tree can be degenerated to a list. Union(1, 2), Union(2, 3), Union(3, 4)\u0026hellip;.. QuickSelect Quick Select can be used to find the k-th largest element in an array with time complexity of O(n).\nThe average time complexity is O(n)\nAssume we keep on executing quick select on half of the array: n + (n / 2) + (n / 4) + \u0026hellip; = 2n The worst case time complexity is O(n^2)\nAssume each quick select we can only eliminate one element, therefore, we need to run the algorithm n times 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public int findKthLargest(int[] nums, int k) { return quickSelect(nums, 0, nums.length - 1, nums.length - k); } public void swap(int[] nums, int left, int right) { int temp = nums[left]; nums[left] = nums[right]; nums[right] = temp; } public int quickSelect(int[] nums, int left, int right, int k) { int ptr = left; int pivot = nums[right]; for (int i = left; i \u0026lt; right; i++) { if (nums[i] \u0026lt; pivot) { swap(nums, i, ptr); ptr++; } } swap(nums, ptr, right); if (k \u0026lt; ptr) return quickSelect(nums, left, ptr - 1, k); if (k \u0026gt; ptr) return quickSelect(nums, ptr + 1, right, k); return nums[k]; } Balanced Binary Search Tree AVL Tree (Adelson-Velsky and Landis Tree)\nIt is a self-balancing binary search tree. In an AVL tree, the heights of the two child subtrees of any node differ by at most one. If at any time they differ by more than one, re-balancing is done to restore this property. Search / Delete / Insert: O(logn) Red Black Tree\nA node is either red or black. The root and leaves (NIL) are black. All paths from a node to its NIL descendants contain the same number of black nodes. The longest path is no more than twice the length of the shortest path. Search / Delete / Insert: O(logn) Red Black Trees provide faster insertion and removal operations than AVL trees as fewer rotations are done due to relatively relaxed balancing.\nGraph algorithms are used to solve the problems of representing graphs as networks like airline flights, how the Internet is connected, or social network connectivity on Facebook. They are also popular in NLP and machine learning fields to form networks (Ejonavi 2020). In this post, I will talk about some important graph algorithms with basic ideas under the hood, specific implementations, and complexity analysis.\nBreadth First Search (BFS) Idea for breadth-first-search: Start from node s and visit all nodes with distance i to s in iteration i.\nImplementation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 public class BFS { int nodeNum; LinkedList\u0026lt;Integer\u0026gt;[] adjacencyList; public BFS(int nodeNum) { this.nodeNum = nodeNum; adjacencyList = new LinkedList[nodeNum]; for (int i = 0; i \u0026lt; nodeNum; i++) { adjacencyList[i] = new LinkedList\u0026lt;\u0026gt;(); } } public void addEdge(int parent, int child) { adjacencyList[parent].add(child); } public void BFS(int startingNode) { ArrayList\u0026lt;ArrayList\u0026lt;Integer\u0026gt;\u0026gt; outcome = new ArrayList\u0026lt;\u0026gt;(); ArrayList\u0026lt;Boolean\u0026gt; visited = new ArrayList\u0026lt;\u0026gt;(Collections.nCopies(nodeNum, false)); LinkedList\u0026lt;Integer\u0026gt; myQue = new LinkedList\u0026lt;\u0026gt;(); myQue.push(startingNode); visited.set(startingNode, true); while (!myQue.isEmpty()) { ArrayList\u0026lt;Integer\u0026gt; oneLayer = new ArrayList\u0026lt;\u0026gt;(); int size = myQue.size(); for (int i = 0; i \u0026lt; size; i++) { int s = myQue.remove(); oneLayer.add(s); for (int n : adjacencyList[s]) { if (!visited.get(n)) { visited.set(n, true); myQue.add(n); } } } outcome.add(oneLayer); } System.out.println(outcome); } } Depth First Search (DFS) Idea for depth-first-search: Whenever you visit a vertex, explore in the next step one of its non-visited neighbors.\nImplementation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public class DFS { LinkedList\u0026lt;Integer\u0026gt;[] adjacencyList; ArrayList\u0026lt;Boolean\u0026gt; visited; public DFS(int nodeNum) { adjacencyList = new LinkedList[nodeNum]; for (int i = 0; i \u0026lt; nodeNum; i++) { adjacencyList[i] = new LinkedList\u0026lt;\u0026gt;(); } visited = new ArrayList\u0026lt;\u0026gt;(Collections.nCopies(nodeNum, false)); } public void addEdge(int parent, int child) { adjacencyList[parent].add(child); } public void DFS(int startingNode) { System.out.println(startingNode); visited.set(startingNode, true); for (int n : adjacencyList[startingNode]) { if (!visited.get(n)) { DFS(n); } } } } Dijkstra Algorithm Dijkstra\u0026rsquo;s algorithm is a dynamic programming-like strategy. It checks all the outgoing edges of x to see whether there is a better path from s to some unknown vertex through x.\nAt most V deleteMin and insert operations, and at most E decreaseKey operations.\nTDijkstra = O(E * TdecreaseKey(n) + V * (TdeleteMin(n) + Tinsert(n))) Original: O(E + V2) Binary Heap: O((E + V)log(V)) Fibonacci Heap: O(E + Vlog(V)) Implementation:\nClass Declaration\n1 2 3 4 5 6 7 8 9 10 11 12 class Node implements Comparable\u0026lt;Node\u0026gt; { int node; int cost; Node(int node, int cost) { this.node = node; this.cost = cost; } @Override public int compareTo(Node o) { return this.cost - o.cost; } } Implementation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 public class Dijkstra { LinkedList\u0026lt;Node\u0026gt;[] adjacencyList; ArrayList\u0026lt;Boolean\u0026gt; visited; int nodeNum; public Dijkstra(int nodeNum) { adjacencyList = new LinkedList[nodeNum]; for (int i = 0; i \u0026lt; nodeNum; i++) { adjacencyList[i] = new LinkedList\u0026lt;\u0026gt;(); } visited = new ArrayList\u0026lt;\u0026gt;(Collections.nCopies(nodeNum, false)); this.nodeNum = nodeNum; } public void addEdge(int parent, int child, int cost) { adjacencyList[parent].add(new Node(child, cost)); } public void dijkstra(int startingNode) { PriorityQueue\u0026lt;Node\u0026gt; myQue = new PriorityQueue\u0026lt;\u0026gt;(); ArrayList\u0026lt;Integer\u0026gt; distance = new ArrayList\u0026lt;\u0026gt;(Collections.nCopies(nodeNum, Integer.MAX_VALUE)); // Initialisation myQue.add(new Node(startingNode, 0)); distance.set(startingNode, 0); while (!myQue.isEmpty()) { int u = myQue.poll().node; visited.set(u, true); for (Node n : adjacencyList[u]) { if (!visited.get(n.node)) { int v = n.node; int weight = n.cost; if (distance.get(v) \u0026gt; distance.get(u) + weight) { distance.set(v, distance.get(u) + weight); myQue.add(new Node(v, distance.get(v))); } } } } // Print shortest distances stored in dist[] for (int i = 0; i \u0026lt; distance.size(); i++) { System.out.println(i + \u0026#34; \u0026#34; + distance.get(i)); } } } Minimum Spanning Tree and Algorithms Minimum Spanning Tree: A spanning tree of a graph G = (V, E) is a subset of edges from E forming a tree connecting all vertices of V. A minimum spanning tree minimizes the total length over all possible spanning trees.\nPrim\u0026rsquo;s Algorithm: Prim’s algorithm starts from one vertex and grows the rest of the tree one edge at a time until all vertices are included. This algorithm works in a greedy manner such that it picks the best local optimum which is the shortest path to connect with a new unvisited node in each iteration. The time complexity of Prim\u0026rsquo;s algorithm is O(E + Vlog(V)) when using Fibonacci heaps for implementation of the priority queue.\nImplementation:\nClass Declaration\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Node implements Comparable\u0026lt;Node\u0026gt; { int node; int cost; Node(int node, int cost) { this.node = node; this.cost = cost; } @Override public int compareTo(Node o) { return this.cost - o.cost; } @Override public String toString() { return \u0026#34;Node{\u0026#34; + \u0026#34;node=\u0026#34; + node + \u0026#34;, cost=\u0026#34; + cost + \u0026#39;}\u0026#39;; } } Implementation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 public class Prim { LinkedList\u0026lt;Node\u0026gt;[] adjacencyList; ArrayList\u0026lt;Boolean\u0026gt; visited; int nodeNum; public Prim(int nodeNum) { adjacencyList = new LinkedList[nodeNum]; for (int i = 0; i \u0026lt; nodeNum; i++) { adjacencyList[i] = new LinkedList\u0026lt;\u0026gt;(); } visited = new ArrayList\u0026lt;\u0026gt;(Collections.nCopies(nodeNum, false)); this.nodeNum = nodeNum; } public void addEdge(int parent, int child, int cost) { adjacencyList[parent].add(new Node(child, cost)); adjacencyList[child].add(new Node(parent, cost)); } public ArrayList\u0026lt;Integer\u0026gt; prim(int startingNode) { ArrayList\u0026lt;Integer\u0026gt; outcome = new ArrayList\u0026lt;\u0026gt;(Collections.nCopies(nodeNum, -1)); PriorityQueue\u0026lt;Node\u0026gt; myQue = new PriorityQueue\u0026lt;\u0026gt;(); ArrayList\u0026lt;Integer\u0026gt; distance = new ArrayList\u0026lt;\u0026gt;(Collections.nCopies(nodeNum, Integer.MAX_VALUE)); myQue.add(new Node(startingNode, 0)); distance.set(startingNode, 0); outcome.set(startingNode, startingNode); while (!myQue.isEmpty()) { int node = myQue.poll().node; visited.set(node, true); for (Node neighbour : adjacencyList[node]) { int weight = neighbour.cost; int vertex = neighbour.node; if (!visited.get(vertex) \u0026amp;\u0026amp; distance.get(vertex) \u0026gt; weight) { distance.set(vertex, weight); outcome.set(vertex, node); myQue.add(new Node(vertex, distance.get(vertex))); } } } return outcome; } } Kruskal\u0026rsquo;s Algorithm: Kruskal\u0026rsquo;s algorithm repeatedly considers the lightest remaining edge and tests whether its two endpoints lie within the same connected component. This algorithm works by sorting edges based on their weight. The time complexity of Kruskal\u0026rsquo;s algorithm is O(Elog(E)).\n2E find operation and (n - 1) union operation. O(2E + Vlog(V)) | Sorting takes O(Elog(E)) | E \u0026gt; V O(Elog(E)) Prim\u0026rsquo;s algorithm is more efficient for dense graphs, while Kruskal\u0026rsquo;s algorithm is more efficient for sparse graphs.\nReference GeeksforGeeks 2022, Introduction to Amortized Analysis, GeeksforGeeks, viewed 11 November 2022, https://www.geeksforgeeks.org/introduction-to-amortized-analysis/.\nSkiena, S 2012, The Algorithm Design Manual, 2nd edn, Springer Publishing, New York, USA.\nWhat Is A Heap Data Structure In Java 2022, What Is A Heap Data Structure In Java, SoftwareTestingHelp, viewed 27 November 2022, https://www.softwaretestinghelp.com/heap-data-structure-in-java/.\nEjonavi, J 2020, Algorithms 101: How to use graph algorithms, Educative, viewed 1 October 2022, https://www.educative.io/blog/graph-algorithms-tutorial.\n","date":"2022-12-12T09:44:14+10:30","image":"https://jiajun2001.github.io/p/algorithm-and-data-structure-theory-study/structure_hu_d841b7fdf168fa59.png","permalink":"https://jiajun2001.github.io/p/algorithm-and-data-structure-theory-study/","title":"Algorithm and Data Structure Theory Study"}]